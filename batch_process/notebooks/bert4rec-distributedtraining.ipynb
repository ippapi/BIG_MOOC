{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11755867,"sourceType":"datasetVersion","datasetId":7342438},{"sourceId":391891,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":310764,"modelId":331134},{"sourceId":398907,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":326578,"modelId":347481}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass BertTrainDataset(Dataset):\n    def __init__(self, u2seq, max_len, mask_prob, mask_token, num_items, rng):\n        self.u2seq = u2seq\n        self.users = sorted(self.u2seq.keys())\n        self.max_len = max_len\n        self.mask_prob = mask_prob\n        self.mask_token = mask_token\n        self.num_items = num_items\n        self.rng = rng\n\n    def __len__(self):\n        return len(self.users)\n\n    def __getitem__(self, index):\n        user = self.users[index]\n        seq = self._getseq(user)\n\n        tokens = []\n        labels = []\n        for s in seq:\n            prob = self.rng.random()\n            if prob < self.mask_prob:\n                prob /= self.mask_prob\n\n                if prob < 0.8:\n                    tokens.append(self.mask_token)\n                elif prob < 0.9:\n                    tokens.append(self.rng.randint(1, self.num_items))\n                else:\n                    tokens.append(s)\n\n                labels.append(s)\n            else:\n                tokens.append(s)\n                labels.append(0)\n\n        tokens = tokens[-self.max_len:]\n        labels = labels[-self.max_len:]\n\n        mask_len = self.max_len - len(tokens)\n\n        tokens = [0] * mask_len + tokens\n        labels = [0] * mask_len + labels\n\n        return torch.LongTensor(tokens), torch.LongTensor(labels)\n\n    def _getseq(self, user):\n        return self.u2seq[user]\n\n\n\nclass BertEvalDataset(Dataset):\n    def __init__(self, u2seq, u2answer, max_len, mask_token, negative_samples):\n        self.u2seq = u2seq\n        self.users = sorted(self.u2seq.keys())\n        self.u2answer = u2answer\n        self.max_len = max_len\n        self.mask_token = mask_token\n        self.negative_samples = negative_samples\n\n    def __len__(self):\n        return len(self.users)\n\n    def __getitem__(self, index):\n        user = self.users[index]\n        seq = self.u2seq[user]\n        answer = self.u2answer[user]\n        negs = self.negative_samples[user]\n\n        candidates = answer + negs\n        labels = [1] * len(answer) + [0] * len(negs)\n\n        seq = seq + [self.mask_token]\n        seq = seq[-self.max_len:]\n        padding_len = self.max_len - len(seq)\n        seq = [0] * padding_len + seq\n\n        return torch.LongTensor(seq), torch.LongTensor(candidates), torch.LongTensor(labels)\n\nclass PredictDataset(Dataset):\n    def __init__(self, u2seq, max_len, mask_token):\n        self.u2seq = u2seq\n        self.users = sorted(self.u2seq.keys())\n        self.max_len = max_len\n        self.mask_token = mask_token\n        \n    def __len__(self):\n        return len(self.users)\n\n    def __getitem__(self, index):\n        user = self.users[index]\n        seq = self.u2seq[user]\n        history_items = list(set(seq))\n\n        seq = seq + [self.mask_token]\n        seq = seq[-self.max_len:]\n        padding_len = self.max_len - len(seq)\n        seq = [0] * padding_len + seq\n\n        return {\n            'seq': torch.LongTensor(seq),\n            'history': torch.LongTensor(history_items)  \n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:34:25.192356Z","iopub.execute_input":"2025-05-19T06:34:25.192640Z","iopub.status.idle":"2025-05-19T06:34:29.943733Z","shell.execute_reply.started":"2025-05-19T06:34:25.192617Z","shell.execute_reply":"2025-05-19T06:34:29.942612Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Data loader","metadata":{}},{"cell_type":"code","source":"import random\nfrom tqdm import trange\nfrom pathlib import Path\nimport pickle\nimport numpy as np\n\nclass RandomNegativeSampler():\n    def __init__(self, train, val, test, user_count, item_count, sample_size, seed, save_dir):\n        self.train = train\n        self.val = val\n        self.test = test\n        self.user_count = user_count\n        self.item_count = item_count\n        self.sample_size = sample_size\n        self.seed = seed\n        self.save_dir = save_dir\n        self.save_path = os.path.join(self.save_dir, 'negative_sample-sample_size{}-seed{}.pkl'.format(self.sample_size, self.seed))\n        \n    def generate_negative_samples(self):\n        assert self.seed is not None, 'Specify seed for random sampling'\n        np.random.seed(self.seed)\n        negative_samples = {}\n        print('Sampling negative items')\n        for user in trange(self.user_count):\n            if isinstance(self.train[user][1], tuple):\n                seen = set(x[0] for x in self.train[user])\n                seen.update(x[0] for x in self.val[user])\n                seen.update(x[0] for x in self.test[user])\n            else:\n                seen = set(self.train[user])\n                seen.update(self.val[user])\n                seen.update(self.test[user])\n\n            samples = []\n            for _ in range(self.sample_size):\n                item = np.random.choice(self.item_count) + 1\n                while item in seen or item in samples:\n                    item = np.random.choice(self.item_count) + 1\n                samples.append(item)\n\n            negative_samples[user] = samples\n\n        return negative_samples\n\n    def get_negative_samples(self):\n        \"\"\"Lấy negative samples từ file nếu đã có, nếu không thì tạo mới và lưu vào file.\"\"\"\n        # Kiểm tra xem file đã tồn tại chưa\n        if os.path.exists(self.save_path):\n            print(\"Negatives samples exist. Loading.\")\n            with open(self.save_path, \"rb\") as f:\n                negative_samples = pickle.load(f)\n        else:\n            print(\"Negative samples don't exist. Generating.\")\n            negative_samples = self.generate_negative_samples()\n            os.makedirs(os.path.dirname(self.save_path), exist_ok=True)\n            \n            with open(self.save_path, \"wb\") as f:\n                pickle.dump(negative_samples, f)\n            print(f\"Saved negative samples to {self.save_path}\")\n\n        return negative_samples\n\nclass BertDataloader():\n    def __init__(self, args, dataset):\n        self.args = args\n        seed = args.dataloader_random_seed\n        self.rng = random.Random(seed)\n        self.train = dataset['train']\n        self.val = dataset['val']\n        self.test = dataset['test']\n        self.user_count = dataset['user_count']\n        self.item_count = dataset['item_count']\n        self.max_len = args.bert_max_len\n        self.mask_prob = args.bert_mask_prob\n        self.CLOZE_MASK_TOKEN = self.item_count + 1\n\n        train_negative_sampler = RandomNegativeSampler(self.train, self.val, self.test,\n                                                       self.user_count, self.item_count,\n                                                       args.train_negative_sample_size,\n                                                       args.train_negative_sampling_seed,\n                                                       args.data_dir\n                                                      )\n        test_negative_sampler = RandomNegativeSampler(self.train, self.val, self.test,\n                                                      self.user_count, self.item_count,\n                                                      args.test_negative_sample_size,\n                                                      args.test_negative_sampling_seed,\n                                                      args.data_dir\n                                                      )\n\n        self.train_negative_samples = train_negative_sampler.get_negative_samples()\n        self.test_negative_samples = test_negative_sampler.get_negative_samples()\n\n        self.predict = {}\n        for key in dataset['train'].keys():\n            self.predict[key] = (\n                dataset['train'].get(key, []) + \n                dataset['val'].get(key, []) +\n                dataset['test'].get(key, [])\n            )\n\n    def get_pytorch_dataloaders(self):\n        train_loader = self.get_train_loader()\n        val_loader = self.get_val_loader()\n        test_loader = self.get_test_loader()\n        if self.args.is_distributed:\n            return train_loader, val_loader, test_loader\n\n        return train_loader, val_loader, test_loader\n\n    def get_train_loader(self):\n        dataset = self._get_train_dataset()\n        if self.args.is_distributed:\n            train_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n        else:\n            train_sampler = None\n        dataloader = DataLoader(dataset, batch_size=self.args.train_batch_size, sampler=train_sampler,\n                                           shuffle=(train_sampler is None), pin_memory=True)\n        return dataloader\n    def _get_train_dataset(self):\n        dataset = BertTrainDataset(self.train, self.max_len, self.mask_prob, self.CLOZE_MASK_TOKEN, self.item_count, self.rng)\n        return dataset\n\n    def get_val_loader(self):\n        return self._get_eval_loader(mode='val')\n\n    def get_test_loader(self):\n        return self._get_eval_loader(mode='test')\n\n    def _get_eval_loader(self, mode):\n        batch_size = self.args.val_batch_size if mode == 'val' else self.args.test_batch_size\n        dataset = self._get_eval_dataset(mode)\n        dataloader = DataLoader(dataset, batch_size=batch_size,\n                                           shuffle=False, pin_memory=True)\n        return dataloader\n\n    def _get_eval_dataset(self, mode):\n        answers = self.val if mode == 'val' else self.test\n        dataset = BertEvalDataset(self.train, answers, self.max_len, self.CLOZE_MASK_TOKEN, self.test_negative_samples)\n        return dataset\n\n    def get_predict_loader(self):\n        dataset = PredictDataset(self.predict,  self.max_len, self.CLOZE_MASK_TOKEN)\n        dataloader = DataLoader(dataset, batch_size=self.args.test_batch_size,\n                                           shuffle=False, pin_memory=True, collate_fn=lambda x: {\n                                               'seq': torch.stack([item['seq'] for item in x]),\n                                               'history': [item['history'] for item in x]\n                                           })\n        return dataloader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:37:31.646047Z","iopub.execute_input":"2025-05-19T06:37:31.646947Z","iopub.status.idle":"2025-05-19T06:37:31.667838Z","shell.execute_reply.started":"2025-05-19T06:37:31.646916Z","shell.execute_reply":"2025-05-19T06:37:31.666918Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass PositionalEmbedding(nn.Module):\n    def __init__(self, max_len, d_model):\n        super().__init__()\n\n        # Compute the positional encodings once in log space.\n        self.pe = nn.Embedding(max_len, d_model)\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        return self.pe.weight.unsqueeze(0).repeat(batch_size, 1, 1)\n\n\nclass TokenEmbedding(nn.Embedding):\n    def __init__(self, vocab_size, embed_size=512):\n        super().__init__(vocab_size, embed_size, padding_idx=0)\n\nclass BERTEmbedding(nn.Module):\n    def __init__(self,\n                 vocab_size,\n                 embed_size,\n                 max_len,\n                 course_embeddings=None,\n                 dropout=0.1):\n        super().__init__()\n        self.token = TokenEmbedding(vocab_size=vocab_size,\n                                    embed_size=embed_size)\n        self.position = PositionalEmbedding(max_len=max_len,\n                                            d_model=embed_size)\n        if course_embeddings is not None:\n            self.metapath_embed = nn.Embedding.from_pretrained(\n                torch.FloatTensor(course_embeddings), freeze=False)\n            self.fusion_proj = nn.Linear(embed_size * 2, embed_size)\n        else:\n            self.metapath_embed = None\n\n        self.dropout = nn.Dropout(p=dropout)\n        self.embed_size = embed_size\n\n    def forward(self, sequence):\n        x = self.token(sequence) +  self.position(sequence)\n        if self.metapath_embed is not None:\n            meta_embed = self.metapath_embed(sequence)\n            x = torch.cat([x, meta_embed], dim=-1)\n            x = self.fusion_proj(x)\n        return self.dropout(x)\n\nclass Attention(nn.Module):\n    def forward(self, query, key, value, mask=None, dropout=None):\n        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n                 / math.sqrt(query.size(-1))\n\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        p_attn = F.softmax(scores, dim=-1)\n\n        if dropout is not None:\n            p_attn = dropout(p_attn)\n\n        return torch.matmul(p_attn, value), p_attn\n\n\nclass MultiHeadedAttention(nn.Module):\n\n    def __init__(self, h, d_model, dropout=0.1):\n        super().__init__()\n        assert d_model % h == 0\n\n        # We assume d_v always equals d_k\n        self.d_k = d_model // h\n        self.h = h\n\n        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model)\n                                            for _ in range(3)])\n        self.output_linear = nn.Linear(d_model, d_model)\n        self.attention = Attention()\n\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n\n        # 1) Do all the linear projections in batch from d_model => h x d_k\n        query, key, value = [\n            layer(x).view(batch_size, -1,\n                          self.h, self.d_k).transpose(1, 2)\n            for layer, x in zip(self.linear_layers, (query, key, value))\n        ]\n\n        # 2) Apply attention on all the projected vectors in batch.\n        x, attn = self.attention(query, key, value,\n                                 mask=mask, dropout=self.dropout)\n\n        # 3) \"Concat\" using a view and apply a final linear.\n        x = x.transpose(1, 2).contiguous().view(batch_size, -1,\n                                                self.h * self.d_k)\n\n        return self.output_linear(x)\n\n\nclass GELU(nn.Module):\n    def forward(self, x):\n        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) *\n                                         (x + 0.044715 * torch.pow(x, 3))))\n\n\nclass PositionwiseFeedForward(nn.Module):\n\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = nn.Linear(d_model, d_ff)\n        self.w_2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.activation = GELU()\n\n    def forward(self, x):\n        return self.w_2(self.dropout(self.activation(self.w_1(x))))\n\n\nclass LayerNorm(nn.Module):\n\n    def __init__(self, features, eps=1e-6):\n        super(LayerNorm, self).__init__()\n        self.a_2 = nn.Parameter(torch.ones(features))\n        self.b_2 = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n\n\nclass SublayerConnection(nn.Module):\n\n    def __init__(self, size, dropout):\n        super(SublayerConnection, self).__init__()\n        self.norm = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, sublayer):\n        \"Apply residual connection to any sublayer with the same size.\"\n        return x + self.dropout(sublayer(self.norm(x)))\n\nclass SublayerConnection(nn.Module):\n\n    def __init__(self, size, dropout):\n        super(SublayerConnection, self).__init__()\n        self.norm = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, sublayer):\n        \"Apply residual connection to any sublayer with the same size.\"\n        return x + self.dropout(sublayer(self.norm(x)))\n\nclass TransformerBlock(nn.Module):\n\n    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):\n\n        super().__init__()\n        self.attention = MultiHeadedAttention(h=attn_heads,\n                                              d_model=hidden,\n                                              dropout=dropout)\n\n        self.feed_forward = PositionwiseFeedForward(d_model=hidden,\n                                                    d_ff=feed_forward_hidden,\n                                                    dropout=dropout)\n\n        self.input_sublayer = SublayerConnection(size=hidden,\n                                                 dropout=dropout)\n\n        self.output_sublayer = SublayerConnection(size=hidden,\n                                                  dropout=dropout)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x, mask):\n        x = self.input_sublayer(x,\n                                lambda _x: self.attention.forward(_x, _x, _x,\n                                                                  mask=mask))\n        x = self.output_sublayer(x, self.feed_forward)\n        return self.dropout(x)\n\nclass BERT4Rec(nn.Module):\n    def __init__(self, args, course_embeddings=None):\n        super().__init__()\n\n        max_len = args.bert_max_len\n        num_items = args.num_items\n        n_layers = args.bert_num_blocks\n        heads = args.bert_num_heads\n        self.vocab_size = num_items + 2\n        hidden = args.bert_hidden_units\n        self.hidden = hidden\n        dropout = args.bert_dropout\n\n        # embedding for BERT, sum of positional, token, course pretrained embeddings\n        self.embedding = BERTEmbedding(vocab_size=self.vocab_size,\n                                       embed_size=self.hidden,\n                                       max_len=max_len,\n                                       course_embeddings=course_embeddings,\n                                       dropout=dropout\n                                       )\n\n        # multi-layers transformer blocks, deep network\n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlock(hidden, heads, hidden * 4, dropout)\n            for _ in range(n_layers)\n        ])\n\n        self.out = nn.Linear(self.hidden, args.num_items + 1)\n\n    def forward(self, x):\n        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n\n        # embedding the indexed sequence to sequence of vectors\n        x = self.embedding(x)\n\n        # running over multiple transformer blocks\n        for transformer in self.transformer_blocks:\n            x = transformer.forward(x, mask)\n\n        x = self.out(x)\n        \n        return x\n\n    def init_weights(self):\n        pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:37:36.308032Z","iopub.execute_input":"2025-05-19T06:37:36.308369Z","iopub.status.idle":"2025-05-19T06:37:36.336483Z","shell.execute_reply.started":"2025-05-19T06:37:36.308344Z","shell.execute_reply":"2025-05-19T06:37:36.335350Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"import os\nimport logging\nimport csv\nfrom collections import OrderedDict\nimport numpy as np\nimport pandas as pd\nimport ast\nfrom collections import defaultdict\nimport ast\n\ndef create_log_id(dir_path):\n    log_count = 0\n    file_path = os.path.join(dir_path, 'log{:d}.log'.format(log_count))\n    while os.path.exists(file_path):\n        log_count += 1\n        file_path = os.path.join(dir_path, 'log{:d}.log'.format(log_count))\n    return log_count\n\n\ndef logging_config(folder=None, name=None,\n                   level=logging.DEBUG,\n                   console_level=logging.DEBUG,\n                   no_console=True):\n\n    if not os.path.exists(folder):\n        os.makedirs(folder)\n    for handler in logging.root.handlers:\n        logging.root.removeHandler(handler)\n    logging.root.handlers = []\n    logpath = os.path.join(folder, name + \".log\")\n    print(\"All logs will be saved to %s\" %logpath)\n\n    logging.root.setLevel(level)\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    logfile = logging.FileHandler(logpath)\n    logfile.setLevel(level)\n    logfile.setFormatter(formatter)\n    logging.root.addHandler(logfile)\n\n    if not no_console:\n        logconsole = logging.StreamHandler()\n        logconsole.setLevel(console_level)\n        logconsole.setFormatter(formatter)\n        logging.root.addHandler(logconsole)\n    return folder\n\ndef early_stopping(recall_list, stopping_steps):\n    best_recall = max(recall_list)\n    best_step = recall_list.index(best_recall)\n    if len(recall_list) - best_step - 1 >= stopping_steps:\n        should_stop = True\n    else:\n        should_stop = False\n    return best_recall, should_stop\n\n\ndef save_checkpoint(model_dir, model, optimizer, current_epoch, best_recall, best_epoch, metrics_list, epoch_list):\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    checkpoint_file = os.path.join(model_dir, 'checkpoint_epoch{}.pth'.format(current_epoch))\n    torch.save({'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'epoch': current_epoch,\n                'best_recall': best_recall,\n                'best_epoch': best_epoch,\n                'metrics_list': metrics_list,\n                'epoch_list': epoch_list\n               }, checkpoint_file)\n\ndef load_model(model, model_path):\n    checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.eval()\n    return model\n\ndef recalls_and_ndcgs_for_ks(scores, labels, ks):\n    metrics = {}\n\n    scores = scores\n    labels = labels\n    answer_count = labels.sum(1)\n\n    labels_float = labels.float()\n    rank = (-scores).argsort(dim=1)\n    cut = rank\n    for k in sorted(ks, reverse=True):\n       cut = cut[:, :k]\n       hits = labels_float.gather(1, cut)\n       metrics['Recall@%d' % k] = \\\n           (hits.sum(1) / torch.min(torch.Tensor([k]).to(labels.device), labels.sum(1).float())).mean().cpu().item()\n\n       position = torch.arange(2, 2+k)\n       weights = 1 / torch.log2(position.float())\n       dcg = (hits * weights.to(hits.device)).sum(1)\n       idcg = torch.Tensor([weights[:min(int(n), k)].sum() for n in answer_count]).to(dcg.device)\n       ndcg = (dcg / idcg).mean()\n       metrics['NDCG@%d' % k] = ndcg.cpu().item()\n\n    return metrics\n\ndef create_dataset(args):\n    num_users = 0\n    num_courses = 0\n    train = defaultdict(list)\n    val = defaultdict(list)\n    test = defaultdict(list)\n\n    def load_train(path, storage):\n        nonlocal num_users, num_courses\n        df = pd.read_csv(path)\n        for _, row in df.iterrows():\n            user = int(row['user'])\n            courses = ast.literal_eval(row['feature'])\n            courses = [course + 1 for course in courses]\n            storage[user].extend(courses)\n            num_users = max(num_users, user)\n            if courses:\n                num_courses = max(num_courses, max(courses))\n\n    def load_single_label_file(path, label_column, storage):\n        nonlocal num_users, num_courses\n        df = pd.read_csv(path)\n        for _, row in df.iterrows():\n            user = int(row['user'])\n            course = int(row[label_column])\n            storage[user].append(course + 1)\n            num_users = max(num_users, user)\n            num_courses = max(num_courses, course)\n\n    data_dir = args.data_dir\n    load_train(os.path.join(data_dir, 'train_df.csv'), train)\n    load_single_label_file(os.path.join(data_dir, 'val_df.csv'), 'val_label', val)\n    load_single_label_file(os.path.join(data_dir, 'test_df.csv'), 'test_label', test)\n    args.num_items = num_courses + 1\n\n    dataset = {'train': train,'val': val,'test': test,'user_count': num_users + 1,'item_count': num_courses + 1}\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:37:42.588573Z","iopub.execute_input":"2025-05-19T06:37:42.588888Z","iopub.status.idle":"2025-05-19T06:37:42.943567Z","shell.execute_reply.started":"2025-05-19T06:37:42.588864Z","shell.execute_reply":"2025-05-19T06:37:42.942576Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Trainer","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom tqdm import tqdm\nimport json\nimport os\nimport pprint as pp\nimport random\nfrom datetime import date, timedelta\nfrom pathlib import Path\nimport sys\nimport numpy as np\n\nimport json\nclass Trainer():\n    def __init__(self, args, model, dataset):\n        self.args = args\n        self.device = args.device\n        self.model = model.to(self.device)\n        self.optimizer = self._create_optimizer()\n        if args.enable_lr_schedule:\n            self.lr_scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=args.decay_step, gamma=args.gamma)\n\n        self.num_epochs = args.num_epochs\n        self.metric_ks = args.metric_ks\n        self.best_metric = args.best_metric\n        self.ce = nn.CrossEntropyLoss(ignore_index=0)\n        \n        self.dataloader = BertDataloader(args, dataset)\n        self.log_save_id = create_log_id(self.args.save_dir)\n        \n        \n    def train(self):\n        if self.args.is_distributed:\n            dist.init_process_group(\"gloo\", timeout=timedelta(seconds=3000))\n            rank = dist.get_rank()\n            is_main_process = (rank == 0)\n            self.model = DDP(self.model)        \n        else:\n            is_main_process = True\n\n        self.train_loader = self.dataloader.get_train_loader()\n        \n\n        # Logging chỉ ở process chính\n        if is_main_process:\n            logging_config(folder=self.args.save_dir, name=f'log{self.log_save_id}', no_console=False)\n            logging.info(self.args)\n            logging.info(self.model)\n\n        self.model.train()\n        best_recall = 0\n        best_epoch = 0\n        recall_list = []\n        min_valid_batch_size = 16\n        should_stop = False\n\n        for epoch in range(1, self.num_epochs + 1):\n            if self.args.is_distributed:\n                self.train_loader.sampler.set_epoch(epoch)\n\n            if self.args.enable_lr_schedule:\n                self.lr_scheduler.step()\n\n            tqdm_dataloader = tqdm(self.train_loader) if is_main_process else self.train_loader\n\n            total_loss = 0.0\n            num_batches = 0\n\n            for batch_idx, batch in enumerate(tqdm_dataloader):\n                try:\n                    batch = [x.to(self.device) for x in batch]\n                except AttributeError:\n                    batch = {k: v.to(self.device) for k, v in batch.items()}\n\n                self.optimizer.zero_grad()\n                loss = self.calculate_loss(batch)\n\n                if torch.isnan(loss).any():\n                    if batch[0].size(0) >= min_valid_batch_size:\n                        if is_main_process:\n                            logging.error(f'ERROR: Epoch {epoch}, batch {batch_idx + 1} Loss is nan.')\n                        if self.args.is_distributed:\n                            dist.destroy_process_group()\n                        sys.exit()\n                    else:\n                        continue\n\n                loss.backward()\n                self.optimizer.step()\n\n                total_loss += loss.item()\n                num_batches += 1\n\n\n            # Tính average_loss: nếu phân tán thì cần reduce từ các GPU\n            average_loss_tensor = torch.tensor([total_loss, num_batches], dtype=torch.float32, device=self.device)\n\n            if self.args.is_distributed:\n                dist.all_reduce(average_loss_tensor, op=dist.ReduceOp.SUM)\n\n            total_loss = average_loss_tensor[0].item()\n            num_batches = average_loss_tensor[1].item()\n            average_loss = total_loss / max(num_batches, 1)\n\n            if is_main_process:\n                logging.info(f'Epoch {epoch:04d} | Average Loss: {average_loss:.4f}')\n\n            dist.barrier()\n                \n            if (epoch % self.args.evaluate_every == 0 or epoch == self.args.num_epochs) and is_main_process:\n                if self.args.is_distributed:\n                    self.model.module.eval()\n                else:\n                    self.model.eval()\n\n                all_metrics = {\n                    \"Recall@1\": [],\n                    \"Recall@5\": [],\n                    \"Recall@10\": [],\n                    \"NDCG@1\": [],\n                    \"NDCG@5\": [],\n                    \"NDCG@10\": []\n                }\n\n                with torch.no_grad():\n                    self.val_loader = self.dataloader.get_val_loader()\n                    tqdm_dataloader = tqdm(self.val_loader)\n                    for batch_idx, batch in enumerate(tqdm_dataloader):\n                        batch_size = batch[0].size(0)\n                        batch = [x.to(self.device) for x in batch]\n\n                        metrics = self.calculate_metrics(batch)\n                        if batch_size >= min_valid_batch_size:\n                            for key in all_metrics:\n                                all_metrics[key].append(metrics[key])\n\n                avg_metrics = {key: sum(values) / len(values) for key, values in all_metrics.items()}\n                recall_list.append(avg_metrics[self.args.best_metric])\n\n                logging.info('Val: Epoch {:04d} | Recall [{:.4f}, {:.4f}, {:.4f}], NDCG [{:.4f}, {:.4f} {:.4f}]'.format(\n                            epoch, avg_metrics['Recall@1'],  avg_metrics['Recall@5'], avg_metrics['Recall@10'],  avg_metrics['NDCG@1'], avg_metrics['NDCG@5'], avg_metrics['NDCG@10']))\n\n                best_recall, should_stop = early_stopping(recall_list, args.stopping_steps)\n                \n\n                if recall_list[-1] == best_recall:\n                    if self.args.is_distributed:\n                        model_state_dict = self.model.module.state_dict()\n                    else:\n                        model_state_dict = self.model.state_dict()\n\n                    model_save_path = os.path.join(self.args.save_dir, 'model', 'best_model.pth')\n                    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n                    torch.save({'model_state_dict': model_state_dict, 'epoch': epoch},\n                               model_save_path)\n                    logging.info(f'Save model at epoch {epoch:04d}!')\n                    best_epoch = epoch\n            if should_stop:\n                    break    \n        dist.destroy_process_group()\n\n    def test(self):\n        logging_config(folder=self.args.save_dir, name=f'log{self.log_save_id}', no_console=False)\n        print('Test best model with test set!')\n        best_model = torch.load(os.path.join(self.args.save_dir, 'model', 'best_model.pth')).get('model_state_dict')\n        self.model.load_state_dict(best_model)\n\n        self.model.eval()\n        all_metrics = {\n            \"Recall@1\": [],\n            \"Recall@5\": [],\n            \"Recall@10\": [],\n            \"NDCG@1\": [],\n            \"NDCG@5\": [],\n            \"NDCG@10\": []\n        }\n        \n        min_test_batch_size = 16\n        \n        self.test_loader = self.dataloader.get_test_loader()\n        with torch.no_grad():\n            tqdm_dataloader = tqdm(self.test_loader)\n            for batch_idx, batch in enumerate(tqdm_dataloader):\n                batch_size = batch[0].size(0)\n                batch = [x.to(self.device) for x in batch]\n                metrics = self.calculate_metrics(batch)\n                if batch_size >= min_test_batch_size:\n                    for key in all_metrics:\n                        all_metrics[key].append(metrics[key])\n\n        avg_metrics = {key: sum(values) / len(values) for key, values in all_metrics.items()}\n        logging.info('Test: Recall [{:.4f}, {:.4f}, {:.4f}], NDCG [{:.4f}, {:.4f} {:.4f}]'.format(avg_metrics['Recall@1'],  avg_metrics['Recall@5'], avg_metrics['Recall@10'],  avg_metrics['NDCG@1'], avg_metrics['NDCG@5'], avg_metrics['NDCG@10']))\n\n    def predict(self, top_n=10):\n        print('Predict with best model.')\n        best_model = torch.load(os.path.join(self.args.save_dir, 'model', 'best_model.pth')).get('model_state_dict')\n        self.model.load_state_dict(best_model)\n        \n        self.model.eval()\n        \n        self.predict_loader = self.dataloader.get_predict_loader()\n        all_preds = []\n        all_scores = []\n        with torch.no_grad():\n            for batch in self.predict_loader:\n                seq = batch['seq'].to(self.device)          # (batch_size, seq_len)\n                history = batch['history']\n                logits = self.model(seq)     \n                last_logits = logits[:, -1, :] \n                \n                for i in range(last_logits.size(0)):\n                    last_logits[i, history[i]] = float('-inf')\n                    \n                topk_scores, topk_items = torch.topk(last_logits, k=top_n, dim=-1)\n                all_preds.extend(topk_items.cpu().tolist())\n                all_scores.extend(topk_scores.cpu().tolist())\n\n        return all_preds, all_scores\n        \n    def _create_optimizer(self):\n        args = self.args\n        if args.optimizer.lower() == 'adam':\n            return optim.Adam(self.model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n        elif args.optimizer.lower() == 'sgd':\n            return optim.SGD(self.model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=args.momentum)\n        else:\n            raise ValueError\n\n    def calculate_loss(self, batch):\n        seqs, labels = batch\n        logits = self.model(seqs)\n\n        logits = logits.view(-1, logits.size(-1))\n        labels = labels.view(-1)\n        loss = self.ce(logits, labels)\n        return loss\n\n    def calculate_metrics(self, batch):\n        seqs, candidates, labels = batch\n        scores = self.model(seqs)\n        scores = scores[:, -1, :]\n        scores = scores.gather(1, candidates)\n\n        metrics = recalls_and_ndcgs_for_ks(scores, labels, self.metric_ks)\n        return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:37:48.736420Z","iopub.execute_input":"2025-05-19T06:37:48.736983Z","iopub.status.idle":"2025-05-19T06:37:48.770463Z","shell.execute_reply.started":"2025-05-19T06:37:48.736955Z","shell.execute_reply":"2025-05-19T06:37:48.769321Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from pyspark.ml.torch.distributor import TorchDistributor\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n    .appName(\"DistributedTorchTrain\") \\\n    .getOrCreate()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:37:54.166628Z","iopub.execute_input":"2025-05-19T06:37:54.167415Z","iopub.status.idle":"2025-05-19T06:38:02.093365Z","shell.execute_reply.started":"2025-05-19T06:37:54.167385Z","shell.execute_reply":"2025-05-19T06:38:02.092204Z"}},"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/05/19 06:37:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"distributor = TorchDistributor(num_processes=2, local_mode=True, use_gpu=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:38:04.047924Z","iopub.execute_input":"2025-05-19T06:38:04.048857Z","iopub.status.idle":"2025-05-19T06:38:05.592189Z","shell.execute_reply.started":"2025-05-19T06:38:04.048817Z","shell.execute_reply":"2025-05-19T06:38:05.590894Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"#### bert_max_len=20, bert_hidden_units=128, bert_mask_prob=0.3","metadata":{}},{"cell_type":"code","source":"from types import SimpleNamespace\nargs = SimpleNamespace(\n    train_batch_size=128,\n    val_batch_size=128,\n    test_batch_size=128,\n    train_negative_sample_size=0,\n    train_negative_sampling_seed=0,\n    test_negative_sample_size=100,\n    test_negative_sampling_seed=98765,\n    is_distributed=True,\n    device='cpu',\n    optimizer='Adam',\n    lr=0.001,\n    enable_lr_schedule=True,\n    weight_decay=0.00,\n    decay_step=25,\n    gamma=1.0,\n    num_epochs=50,\n    evaluate_every=5,\n    metric_ks=[1, 5, 10],\n    best_metric='Recall@10',\n    stopping_steps=5,\n    bert_dropout=0.1,\n    bert_hidden_units=128,\n    bert_mask_prob=0.3,\n    bert_max_len=20,\n    bert_num_blocks=2,\n    bert_num_heads=4,\n    data_dir=\"/kaggle/input/mooc-bert4rec\",\n    save_dir=\"/kaggle/working/\",\n    dataloader_random_seed=2025\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T05:45:01.509102Z","iopub.execute_input":"2025-05-19T05:45:01.510235Z","iopub.status.idle":"2025-05-19T05:45:01.516715Z","shell.execute_reply.started":"2025-05-19T05:45:01.510201Z","shell.execute_reply":"2025-05-19T05:45:01.515601Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"dataset = create_dataset(args)\nmodel = BERT4Rec(args)\ntrainer = Trainer(args, model, dataset)\ndistributor.run(trainer.train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:41:13.149302Z","iopub.execute_input":"2025-05-18T07:41:13.149911Z","iopub.status.idle":"2025-05-18T10:27:18.123026Z","shell.execute_reply.started":"2025-05-18T07:41:13.149884Z","shell.execute_reply":"2025-05-18T10:27:18.121883Z"}},"outputs":[{"name":"stdout","text":"Negatives samples exist. Loading.\nNegatives samples exist. Loading.\nAll logs will be saved to /kaggle/working/log0.log\n2025-05-18 07:41:49,765 - root - INFO - namespace(train_batch_size=128, val_batch_size=128, test_batch_size=128, train_negative_sample_size=0, train_negative_sampling_seed=0, test_negative_sample_size=100, test_negative_sampling_seed=98765, is_distributed=True, device='cpu', optimizer='Adam', lr=0.001, enable_lr_schedule=True, weight_decay=0.0, decay_step=25, gamma=1.0, num_epochs=50, evaluate_every=5, metric_ks=[1, 5, 10], best_metric='Recall@10', stopping_steps=5, bert_dropout=0.1, bert_hidden_units=128, bert_mask_prob=0.3, bert_max_len=20, bert_num_blocks=2, bert_num_heads=4, data_dir='/kaggle/input/mooc-bert4rec', save_dir='/kaggle/working/', dataloader_random_seed=2025, num_items=2828)\n2025-05-18 07:41:49,765 - root - INFO - DistributedDataParallel(\n  (module): BERT4Rec(\n    (embedding): BERTEmbedding(\n      (token): TokenEmbedding(2830, 128, padding_idx=0)\n      (position): PositionalEmbedding(\n        (pe): Embedding(20, 128)\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer_blocks): ModuleList(\n      (0-1): 2 x TransformerBlock(\n        (attention): MultiHeadedAttention(\n          (linear_layers): ModuleList(\n            (0-2): 3 x Linear(in_features=128, out_features=128, bias=True)\n          )\n          (output_linear): Linear(in_features=128, out_features=128, bias=True)\n          (attention): Attention()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=128, out_features=512, bias=True)\n          (w_2): Linear(in_features=512, out_features=128, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): GELU()\n        )\n        (input_sublayer): SublayerConnection(\n          (norm): LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (output_sublayer): SublayerConnection(\n          (norm): LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (out): Linear(in_features=128, out_features=2829, bias=True)\n  )\n)\n/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:214: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:214: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\n100%|██████████| 391/391 [02:54<00:00,  2.24it/s]\n2025-05-18 07:44:44,037 - root - INFO - Epoch 0001 | Average Loss: 6.5802\n100%|██████████| 391/391 [02:55<00:00,  2.22it/s]\n2025-05-18 07:47:39,897 - root - INFO - Epoch 0002 | Average Loss: 5.9782\n100%|██████████| 391/391 [02:55<00:00,  2.22it/s]\n2025-05-18 07:50:35,800 - root - INFO - Epoch 0003 | Average Loss: 5.7553\n100%|██████████| 391/391 [02:55<00:00,  2.23it/s]\n2025-05-18 07:53:30,887 - root - INFO - Epoch 0004 | Average Loss: 5.6156\n100%|██████████| 391/391 [02:55<00:00,  2.22it/s]\n2025-05-18 07:56:26,656 - root - INFO - Epoch 0005 | Average Loss: 5.5211\n100%|██████████| 782/782 [01:54<00:00,  6.81it/s]\n2025-05-18 07:58:21,489 - root - INFO - Val: Epoch 0005 | Recall [0.2858, 0.6216, 0.7768], NDCG [0.2858, 0.4604 0.5108]\n2025-05-18 07:58:21,498 - root - INFO - Save model at epoch 0005!\n100%|██████████| 391/391 [02:55<00:00,  2.23it/s]\n2025-05-18 08:01:16,902 - root - INFO - Epoch 0006 | Average Loss: 5.3630\n100%|██████████| 391/391 [02:55<00:00,  2.23it/s]\n2025-05-18 08:04:12,239 - root - INFO - Epoch 0007 | Average Loss: 5.2954\n100%|██████████| 391/391 [02:55<00:00,  2.23it/s]\n2025-05-18 08:07:07,520 - root - INFO - Epoch 0008 | Average Loss: 5.2501\n100%|██████████| 391/391 [02:53<00:00,  2.26it/s]\n2025-05-18 08:10:00,854 - root - INFO - Epoch 0009 | Average Loss: 5.2151\n100%|██████████| 391/391 [02:53<00:00,  2.26it/s]\n2025-05-18 08:12:53,984 - root - INFO - Epoch 0010 | Average Loss: 5.1814\n100%|██████████| 782/782 [01:53<00:00,  6.91it/s]\n2025-05-18 08:14:47,123 - root - INFO - Val: Epoch 0010 | Recall [0.3255, 0.6696, 0.8152], NDCG [0.3255, 0.5053 0.5527]\n2025-05-18 08:14:47,133 - root - INFO - Save model at epoch 0010!\n100%|██████████| 391/391 [02:56<00:00,  2.22it/s]\n2025-05-18 08:17:43,572 - root - INFO - Epoch 0011 | Average Loss: 5.1542\n100%|██████████| 391/391 [02:57<00:00,  2.21it/s]\n2025-05-18 08:20:40,876 - root - INFO - Epoch 0012 | Average Loss: 5.1383\n100%|██████████| 391/391 [02:55<00:00,  2.23it/s]\n2025-05-18 08:23:36,498 - root - INFO - Epoch 0013 | Average Loss: 5.1195\n100%|██████████| 391/391 [02:54<00:00,  2.24it/s]\n2025-05-18 08:26:31,135 - root - INFO - Epoch 0014 | Average Loss: 5.0958\n100%|██████████| 391/391 [02:59<00:00,  2.17it/s]\n2025-05-18 08:29:31,097 - root - INFO - Epoch 0015 | Average Loss: 5.0769\n100%|██████████| 782/782 [01:53<00:00,  6.90it/s]\n2025-05-18 08:31:24,447 - root - INFO - Val: Epoch 0015 | Recall [0.3402, 0.6824, 0.8270], NDCG [0.3402, 0.5193 0.5663]\n2025-05-18 08:31:24,458 - root - INFO - Save model at epoch 0015!\n100%|██████████| 391/391 [02:53<00:00,  2.25it/s]\n2025-05-18 08:34:18,027 - root - INFO - Epoch 0016 | Average Loss: 5.0600\n100%|██████████| 391/391 [02:54<00:00,  2.24it/s]\n2025-05-18 08:37:12,451 - root - INFO - Epoch 0017 | Average Loss: 5.0470\n100%|██████████| 391/391 [02:55<00:00,  2.22it/s]\n2025-05-18 08:40:08,349 - root - INFO - Epoch 0018 | Average Loss: 5.0299\n100%|██████████| 391/391 [02:57<00:00,  2.20it/s]\n2025-05-18 08:43:06,147 - root - INFO - Epoch 0019 | Average Loss: 5.0243\n100%|██████████| 391/391 [02:56<00:00,  2.22it/s]\n2025-05-18 08:46:02,441 - root - INFO - Epoch 0020 | Average Loss: 5.0137\n100%|██████████| 782/782 [01:54<00:00,  6.84it/s]\n2025-05-18 08:47:56,831 - root - INFO - Val: Epoch 0020 | Recall [0.3493, 0.6892, 0.8308], NDCG [0.3493, 0.5278 0.5738]\n2025-05-18 08:47:56,841 - root - INFO - Save model at epoch 0020!\n100%|██████████| 391/391 [02:54<00:00,  2.25it/s]\n2025-05-18 08:50:50,917 - root - INFO - Epoch 0021 | Average Loss: 5.0065\n100%|██████████| 391/391 [02:52<00:00,  2.26it/s]\n2025-05-18 08:53:43,609 - root - INFO - Epoch 0022 | Average Loss: 4.9918\n100%|██████████| 391/391 [02:53<00:00,  2.26it/s]\n2025-05-18 08:56:36,887 - root - INFO - Epoch 0023 | Average Loss: 4.9761\n100%|██████████| 391/391 [02:56<00:00,  2.21it/s]\n2025-05-18 08:59:33,468 - root - INFO - Epoch 0024 | Average Loss: 4.9677\n100%|██████████| 391/391 [02:59<00:00,  2.17it/s]\n2025-05-18 09:02:33,427 - root - INFO - Epoch 0025 | Average Loss: 4.9632\n100%|██████████| 782/782 [01:56<00:00,  6.74it/s]\n2025-05-18 09:04:29,445 - root - INFO - Val: Epoch 0025 | Recall [0.3540, 0.6970, 0.8368], NDCG [0.3540, 0.5337 0.5791]\n2025-05-18 09:04:29,456 - root - INFO - Save model at epoch 0025!\n100%|██████████| 391/391 [02:57<00:00,  2.20it/s]\n2025-05-18 09:07:27,360 - root - INFO - Epoch 0026 | Average Loss: 4.9553\n100%|██████████| 391/391 [02:56<00:00,  2.21it/s]\n2025-05-18 09:10:24,070 - root - INFO - Epoch 0027 | Average Loss: 4.9525\n100%|██████████| 391/391 [02:54<00:00,  2.24it/s]\n2025-05-18 09:13:18,588 - root - INFO - Epoch 0028 | Average Loss: 4.9469\n100%|██████████| 391/391 [02:56<00:00,  2.22it/s]\n2025-05-18 09:16:15,042 - root - INFO - Epoch 0029 | Average Loss: 4.9343\n100%|██████████| 391/391 [02:55<00:00,  2.23it/s]\n2025-05-18 09:19:10,567 - root - INFO - Epoch 0030 | Average Loss: 4.9402\n100%|██████████| 782/782 [01:54<00:00,  6.81it/s]\n2025-05-18 09:21:05,441 - root - INFO - Val: Epoch 0030 | Recall [0.3602, 0.7040, 0.8397], NDCG [0.3602, 0.5406 0.5848]\n2025-05-18 09:21:05,451 - root - INFO - Save model at epoch 0030!\n100%|██████████| 391/391 [02:56<00:00,  2.22it/s]\n2025-05-18 09:24:01,538 - root - INFO - Epoch 0031 | Average Loss: 4.9298\n100%|██████████| 391/391 [02:56<00:00,  2.21it/s]\n2025-05-18 09:26:58,319 - root - INFO - Epoch 0032 | Average Loss: 4.9185\n100%|██████████| 391/391 [02:56<00:00,  2.22it/s]\n2025-05-18 09:29:54,452 - root - INFO - Epoch 0033 | Average Loss: 4.9126\n100%|██████████| 391/391 [02:54<00:00,  2.24it/s]\n2025-05-18 09:32:48,944 - root - INFO - Epoch 0034 | Average Loss: 4.9092\n100%|██████████| 391/391 [02:55<00:00,  2.23it/s]\n2025-05-18 09:35:43,965 - root - INFO - Epoch 0035 | Average Loss: 4.9051\n100%|██████████| 782/782 [01:54<00:00,  6.82it/s]\n2025-05-18 09:37:38,678 - root - INFO - Val: Epoch 0035 | Recall [0.3644, 0.7028, 0.8394], NDCG [0.3644, 0.5423 0.5868]\n100%|██████████| 391/391 [02:54<00:00,  2.24it/s]\n2025-05-18 09:40:33,568 - root - INFO - Epoch 0036 | Average Loss: 4.9082\n100%|██████████| 391/391 [02:55<00:00,  2.23it/s]\n2025-05-18 09:43:28,790 - root - INFO - Epoch 0037 | Average Loss: 4.8938\n100%|██████████| 391/391 [02:55<00:00,  2.23it/s]\n2025-05-18 09:46:23,998 - root - INFO - Epoch 0038 | Average Loss: 4.8927\n100%|██████████| 391/391 [02:55<00:00,  2.23it/s]\n2025-05-18 09:49:19,598 - root - INFO - Epoch 0039 | Average Loss: 4.8970\n100%|██████████| 391/391 [02:55<00:00,  2.23it/s]\n2025-05-18 09:52:15,262 - root - INFO - Epoch 0040 | Average Loss: 4.8791\n100%|██████████| 782/782 [01:54<00:00,  6.86it/s]\n2025-05-18 09:54:09,345 - root - INFO - Val: Epoch 0040 | Recall [0.3636, 0.7040, 0.8416], NDCG [0.3636, 0.5422 0.5870]\n2025-05-18 09:54:09,355 - root - INFO - Save model at epoch 0040!\n100%|██████████| 391/391 [02:55<00:00,  2.23it/s]\n2025-05-18 09:57:04,732 - root - INFO - Epoch 0041 | Average Loss: 4.8808\n100%|██████████| 391/391 [02:55<00:00,  2.22it/s]\n2025-05-18 10:00:00,501 - root - INFO - Epoch 0042 | Average Loss: 4.8800\n100%|██████████| 391/391 [02:56<00:00,  2.21it/s]\n2025-05-18 10:02:57,113 - root - INFO - Epoch 0043 | Average Loss: 4.8844\n100%|██████████| 391/391 [02:56<00:00,  2.21it/s]\n2025-05-18 10:05:54,107 - root - INFO - Epoch 0044 | Average Loss: 4.8631\n100%|██████████| 391/391 [02:55<00:00,  2.22it/s]\n2025-05-18 10:08:50,049 - root - INFO - Epoch 0045 | Average Loss: 4.8683\n100%|██████████| 782/782 [01:54<00:00,  6.82it/s]\n2025-05-18 10:10:44,656 - root - INFO - Val: Epoch 0045 | Recall [0.3660, 0.7096, 0.8448], NDCG [0.3660, 0.5465 0.5906]\n2025-05-18 10:10:44,666 - root - INFO - Save model at epoch 0045!\n100%|██████████| 391/391 [02:54<00:00,  2.24it/s]\n2025-05-18 10:13:39,390 - root - INFO - Epoch 0046 | Average Loss: 4.8654\n100%|██████████| 391/391 [02:56<00:00,  2.21it/s]\n2025-05-18 10:16:35,974 - root - INFO - Epoch 0047 | Average Loss: 4.8599\n100%|██████████| 391/391 [02:55<00:00,  2.23it/s]\n2025-05-18 10:19:31,146 - root - INFO - Epoch 0048 | Average Loss: 4.8533\n100%|██████████| 391/391 [02:55<00:00,  2.23it/s]\n2025-05-18 10:22:26,588 - root - INFO - Epoch 0049 | Average Loss: 4.8562\n100%|██████████| 391/391 [02:54<00:00,  2.24it/s]\n2025-05-18 10:25:21,474 - root - INFO - Epoch 0050 | Average Loss: 4.8555\n100%|██████████| 782/782 [01:54<00:00,  6.86it/s]\n2025-05-18 10:27:15,493 - root - INFO - Val: Epoch 0050 | Recall [0.3694, 0.7088, 0.8449], NDCG [0.3694, 0.5475 0.5918]\n2025-05-18 10:27:15,503 - root - INFO - Save model at epoch 0050!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"trainer.test()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T10:33:03.625214Z","iopub.execute_input":"2025-05-18T10:33:03.626077Z","iopub.status.idle":"2025-05-18T10:34:14.966846Z","shell.execute_reply.started":"2025-05-18T10:33:03.626049Z","shell.execute_reply":"2025-05-18T10:34:14.966000Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/3823408109.py:168: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  best_model = torch.load(os.path.join(self.args.save_dir, 'model', 'best_model.pth')).get('model_state_dict')\n","output_type":"stream"},{"name":"stdout","text":"All logs will be saved to /kaggle/working/log0.log\nTest best model with test set!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [01:11<00:00, 10.97it/s]\n2025-05-18 10:34:14,962 - root - INFO - Test: Recall [0.2767, 0.6065, 0.7701], NDCG [0.2767, 0.4483 0.5013]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"#### bert_max_len=20, bert_hidden_units=256, bert_mask_prob=0.15","metadata":{}},{"cell_type":"code","source":"from types import SimpleNamespace\nargs = SimpleNamespace(\n    train_batch_size=128,\n    val_batch_size=128,\n    test_batch_size=128,\n    train_negative_sample_size=0,\n    train_negative_sampling_seed=0,\n    test_negative_sample_size=100,\n    test_negative_sampling_seed=98765,\n    is_distributed=True,\n    device='cpu',\n    optimizer='Adam',\n    lr=0.001,\n    enable_lr_schedule=True,\n    weight_decay=0.00,\n    decay_step=25,\n    gamma=1.0,\n    num_epochs=50,\n    evaluate_every=5,\n    metric_ks=[1, 5, 10],\n    best_metric='Recall@10',\n    stopping_steps=5,\n    bert_dropout=0.1,\n    bert_hidden_units=256,\n    bert_mask_prob=0.15,\n    bert_max_len=20,\n    bert_num_blocks=2,\n    bert_num_heads=4,\n    data_dir=\"/kaggle/input/mooc-bert4rec\",\n    save_dir=\"/kaggle/working/\",\n    dataloader_random_seed=2025\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:38:57.057372Z","iopub.execute_input":"2025-05-19T06:38:57.057771Z","iopub.status.idle":"2025-05-19T06:38:57.065109Z","shell.execute_reply.started":"2025-05-19T06:38:57.057746Z","shell.execute_reply":"2025-05-19T06:38:57.064129Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"dataset = create_dataset(args)\nmodel = BERT4Rec(args)\ntrainer = Trainer(args, model, dataset)\ndistributor.run(trainer.train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:39:02.008043Z","iopub.execute_input":"2025-05-19T06:39:02.008415Z","iopub.status.idle":"2025-05-19T12:04:06.389977Z","shell.execute_reply.started":"2025-05-19T06:39:02.008391Z","shell.execute_reply":"2025-05-19T12:04:06.388378Z"}},"outputs":[{"name":"stdout","text":"Negatives samples exist. Loading.\nNegatives samples exist. Loading.\nAll logs will be saved to /kaggle/working/log0.log\n2025-05-19 06:39:36,121 - root - INFO - namespace(train_batch_size=128, val_batch_size=128, test_batch_size=128, train_negative_sample_size=0, train_negative_sampling_seed=0, test_negative_sample_size=100, test_negative_sampling_seed=98765, is_distributed=True, device='cpu', optimizer='Adam', lr=0.001, enable_lr_schedule=True, weight_decay=0.0, decay_step=25, gamma=1.0, num_epochs=50, evaluate_every=5, metric_ks=[1, 5, 10], best_metric='Recall@10', stopping_steps=5, bert_dropout=0.1, bert_hidden_units=256, bert_mask_prob=0.15, bert_max_len=20, bert_num_blocks=2, bert_num_heads=4, data_dir='/kaggle/input/mooc-bert4rec', save_dir='/kaggle/working/', dataloader_random_seed=2025, num_items=2828)\n2025-05-19 06:39:36,121 - root - INFO - DistributedDataParallel(\n  (module): BERT4Rec(\n    (embedding): BERTEmbedding(\n      (token): TokenEmbedding(2830, 256, padding_idx=0)\n      (position): PositionalEmbedding(\n        (pe): Embedding(20, 256)\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer_blocks): ModuleList(\n      (0-1): 2 x TransformerBlock(\n        (attention): MultiHeadedAttention(\n          (linear_layers): ModuleList(\n            (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n          )\n          (output_linear): Linear(in_features=256, out_features=256, bias=True)\n          (attention): Attention()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=256, out_features=1024, bias=True)\n          (w_2): Linear(in_features=1024, out_features=256, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): GELU()\n        )\n        (input_sublayer): SublayerConnection(\n          (norm): LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (output_sublayer): SublayerConnection(\n          (norm): LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (out): Linear(in_features=256, out_features=2829, bias=True)\n  )\n)\n/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:214: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\n  0%|          | 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:214: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\n100%|██████████| 391/391 [06:08<00:00,  1.06it/s]\n2025-05-19 06:45:44,452 - root - INFO - Epoch 0001 | Average Loss: 6.4554\n100%|██████████| 391/391 [06:10<00:00,  1.05it/s]\n2025-05-19 06:51:55,443 - root - INFO - Epoch 0002 | Average Loss: 5.8334\n100%|██████████| 391/391 [06:09<00:00,  1.06it/s]\n2025-05-19 06:58:05,015 - root - INFO - Epoch 0003 | Average Loss: 5.6310\n100%|██████████| 391/391 [06:07<00:00,  1.06it/s]\n2025-05-19 07:04:12,436 - root - INFO - Epoch 0004 | Average Loss: 5.5002\n100%|██████████| 391/391 [06:07<00:00,  1.07it/s]\n2025-05-19 07:10:19,468 - root - INFO - Epoch 0005 | Average Loss: 5.3960\n100%|██████████| 782/782 [03:34<00:00,  3.65it/s]\n2025-05-19 07:13:54,014 - root - INFO - Val: Epoch 0005 | Recall [0.2970, 0.6319, 0.7865], NDCG [0.2970, 0.4713 0.5215]\n2025-05-19 07:13:54,031 - root - INFO - Save model at epoch 0005!\n100%|██████████| 391/391 [06:00<00:00,  1.09it/s]\n2025-05-19 07:19:54,116 - root - INFO - Epoch 0006 | Average Loss: 5.2433\n100%|██████████| 391/391 [05:59<00:00,  1.09it/s]\n2025-05-19 07:25:53,401 - root - INFO - Epoch 0007 | Average Loss: 5.1914\n100%|██████████| 391/391 [06:01<00:00,  1.08it/s]\n2025-05-19 07:31:54,754 - root - INFO - Epoch 0008 | Average Loss: 5.1426\n100%|██████████| 391/391 [05:59<00:00,  1.09it/s]\n2025-05-19 07:37:53,853 - root - INFO - Epoch 0009 | Average Loss: 5.1058\n100%|██████████| 391/391 [05:54<00:00,  1.10it/s]\n2025-05-19 07:43:48,085 - root - INFO - Epoch 0010 | Average Loss: 5.0651\n100%|██████████| 782/782 [03:37<00:00,  3.60it/s]\n2025-05-19 07:47:25,116 - root - INFO - Val: Epoch 0010 | Recall [0.3285, 0.6668, 0.8139], NDCG [0.3285, 0.5050 0.5528]\n2025-05-19 07:47:25,148 - root - INFO - Save model at epoch 0010!\n100%|██████████| 391/391 [05:58<00:00,  1.09it/s]\n2025-05-19 07:53:23,520 - root - INFO - Epoch 0011 | Average Loss: 5.0386\n100%|██████████| 391/391 [06:01<00:00,  1.08it/s]\n2025-05-19 07:59:25,107 - root - INFO - Epoch 0012 | Average Loss: 5.0101\n100%|██████████| 391/391 [05:58<00:00,  1.09it/s]\n2025-05-19 08:05:23,430 - root - INFO - Epoch 0013 | Average Loss: 4.9889\n100%|██████████| 391/391 [05:58<00:00,  1.09it/s]\n2025-05-19 08:11:22,120 - root - INFO - Epoch 0014 | Average Loss: 4.9764\n100%|██████████| 391/391 [05:54<00:00,  1.10it/s]\n2025-05-19 08:17:16,258 - root - INFO - Epoch 0015 | Average Loss: 4.9456\n100%|██████████| 782/782 [03:42<00:00,  3.52it/s]\n2025-05-19 08:20:58,425 - root - INFO - Val: Epoch 0015 | Recall [0.3379, 0.6770, 0.8233], NDCG [0.3379, 0.5150 0.5626]\n2025-05-19 08:20:58,450 - root - INFO - Save model at epoch 0015!\n100%|██████████| 391/391 [05:51<00:00,  1.11it/s]\n2025-05-19 08:26:49,741 - root - INFO - Epoch 0016 | Average Loss: 4.9385\n100%|██████████| 391/391 [05:50<00:00,  1.11it/s]\n2025-05-19 08:32:40,523 - root - INFO - Epoch 0017 | Average Loss: 4.9194\n100%|██████████| 391/391 [05:49<00:00,  1.12it/s]\n2025-05-19 08:38:29,619 - root - INFO - Epoch 0018 | Average Loss: 4.9109\n100%|██████████| 391/391 [05:49<00:00,  1.12it/s]\n2025-05-19 08:44:19,212 - root - INFO - Epoch 0019 | Average Loss: 4.8953\n100%|██████████| 391/391 [05:54<00:00,  1.10it/s]\n2025-05-19 08:50:13,267 - root - INFO - Epoch 0020 | Average Loss: 4.8746\n100%|██████████| 782/782 [03:34<00:00,  3.65it/s]\n2025-05-19 08:53:47,407 - root - INFO - Val: Epoch 0020 | Recall [0.3499, 0.6892, 0.8303], NDCG [0.3499, 0.5274 0.5733]\n2025-05-19 08:53:47,438 - root - INFO - Save model at epoch 0020!\n100%|██████████| 391/391 [05:51<00:00,  1.11it/s]\n2025-05-19 08:59:38,886 - root - INFO - Epoch 0021 | Average Loss: 4.8630\n100%|██████████| 391/391 [05:48<00:00,  1.12it/s]\n2025-05-19 09:05:27,495 - root - INFO - Epoch 0022 | Average Loss: 4.8530\n100%|██████████| 391/391 [05:51<00:00,  1.11it/s]\n2025-05-19 09:11:18,959 - root - INFO - Epoch 0023 | Average Loss: 4.8525\n100%|██████████| 391/391 [05:52<00:00,  1.11it/s]\n2025-05-19 09:17:11,088 - root - INFO - Epoch 0024 | Average Loss: 4.8445\n100%|██████████| 391/391 [05:51<00:00,  1.11it/s]\n2025-05-19 09:23:02,534 - root - INFO - Epoch 0025 | Average Loss: 4.8331\n100%|██████████| 782/782 [03:39<00:00,  3.57it/s]\n2025-05-19 09:26:41,780 - root - INFO - Val: Epoch 0025 | Recall [0.3486, 0.6934, 0.8316], NDCG [0.3486, 0.5292 0.5743]\n2025-05-19 09:26:41,808 - root - INFO - Save model at epoch 0025!\n100%|██████████| 391/391 [05:51<00:00,  1.11it/s]\n2025-05-19 09:32:33,560 - root - INFO - Epoch 0026 | Average Loss: 4.8357\n100%|██████████| 391/391 [05:51<00:00,  1.11it/s]\n2025-05-19 09:38:24,575 - root - INFO - Epoch 0027 | Average Loss: 4.8090\n100%|██████████| 391/391 [05:49<00:00,  1.12it/s]\n2025-05-19 09:44:14,398 - root - INFO - Epoch 0028 | Average Loss: 4.8109\n100%|██████████| 391/391 [05:50<00:00,  1.12it/s]\n2025-05-19 09:50:04,861 - root - INFO - Epoch 0029 | Average Loss: 4.7976\n100%|██████████| 391/391 [05:50<00:00,  1.11it/s]\n2025-05-19 09:55:55,804 - root - INFO - Epoch 0030 | Average Loss: 4.7873\n100%|██████████| 782/782 [03:38<00:00,  3.57it/s]\n2025-05-19 09:59:34,671 - root - INFO - Val: Epoch 0030 | Recall [0.3528, 0.6965, 0.8360], NDCG [0.3528, 0.5327 0.5781]\n2025-05-19 09:59:34,702 - root - INFO - Save model at epoch 0030!\n100%|██████████| 391/391 [05:49<00:00,  1.12it/s]\n2025-05-19 10:05:24,541 - root - INFO - Epoch 0031 | Average Loss: 4.7885\n100%|██████████| 391/391 [05:52<00:00,  1.11it/s]\n2025-05-19 10:11:16,630 - root - INFO - Epoch 0032 | Average Loss: 4.7839\n100%|██████████| 391/391 [05:49<00:00,  1.12it/s]\n2025-05-19 10:17:06,017 - root - INFO - Epoch 0033 | Average Loss: 4.7751\n100%|██████████| 391/391 [05:34<00:00,  1.17it/s]\n2025-05-19 10:22:40,239 - root - INFO - Epoch 0034 | Average Loss: 4.7679\n100%|██████████| 391/391 [05:20<00:00,  1.22it/s]\n2025-05-19 10:28:01,147 - root - INFO - Epoch 0035 | Average Loss: 4.7510\n100%|██████████| 782/782 [03:26<00:00,  3.78it/s]\n2025-05-19 10:31:28,030 - root - INFO - Val: Epoch 0035 | Recall [0.3596, 0.7006, 0.8374], NDCG [0.3596, 0.5382 0.5828]\n2025-05-19 10:31:28,057 - root - INFO - Save model at epoch 0035!\n100%|██████████| 391/391 [05:20<00:00,  1.22it/s]\n2025-05-19 10:36:48,441 - root - INFO - Epoch 0036 | Average Loss: 4.7534\n100%|██████████| 391/391 [05:20<00:00,  1.22it/s]\n2025-05-19 10:42:09,390 - root - INFO - Epoch 0037 | Average Loss: 4.7478\n100%|██████████| 391/391 [05:20<00:00,  1.22it/s]\n2025-05-19 10:47:29,677 - root - INFO - Epoch 0038 | Average Loss: 4.7482\n100%|██████████| 391/391 [05:21<00:00,  1.22it/s]\n2025-05-19 10:52:50,883 - root - INFO - Epoch 0039 | Average Loss: 4.7277\n100%|██████████| 391/391 [05:19<00:00,  1.22it/s]\n2025-05-19 10:58:10,693 - root - INFO - Epoch 0040 | Average Loss: 4.7391\n100%|██████████| 782/782 [03:24<00:00,  3.82it/s]\n2025-05-19 11:01:35,357 - root - INFO - Val: Epoch 0040 | Recall [0.3591, 0.6969, 0.8355], NDCG [0.3591, 0.5362 0.5813]\n100%|██████████| 391/391 [05:19<00:00,  1.22it/s]\n2025-05-19 11:06:55,033 - root - INFO - Epoch 0041 | Average Loss: 4.7205\n100%|██████████| 391/391 [05:19<00:00,  1.22it/s]\n2025-05-19 11:12:14,713 - root - INFO - Epoch 0042 | Average Loss: 4.7162\n100%|██████████| 391/391 [05:19<00:00,  1.23it/s]\n2025-05-19 11:17:33,890 - root - INFO - Epoch 0043 | Average Loss: 4.7133\n100%|██████████| 391/391 [05:20<00:00,  1.22it/s]\n2025-05-19 11:22:54,185 - root - INFO - Epoch 0044 | Average Loss: 4.7085\n100%|██████████| 391/391 [05:19<00:00,  1.22it/s]\n2025-05-19 11:28:14,186 - root - INFO - Epoch 0045 | Average Loss: 4.7060\n100%|██████████| 782/782 [03:25<00:00,  3.81it/s]\n2025-05-19 11:31:39,632 - root - INFO - Val: Epoch 0045 | Recall [0.3581, 0.7026, 0.8386], NDCG [0.3581, 0.5388 0.5831]\n2025-05-19 11:31:39,659 - root - INFO - Save model at epoch 0045!\n100%|██████████| 391/391 [05:31<00:00,  1.18it/s]\n2025-05-19 11:37:10,686 - root - INFO - Epoch 0046 | Average Loss: 4.7055\n100%|██████████| 391/391 [05:48<00:00,  1.12it/s]\n2025-05-19 11:42:58,880 - root - INFO - Epoch 0047 | Average Loss: 4.6996\n100%|██████████| 391/391 [05:47<00:00,  1.13it/s]\n2025-05-19 11:48:46,319 - root - INFO - Epoch 0048 | Average Loss: 4.6818\n100%|██████████| 391/391 [05:49<00:00,  1.12it/s]\n2025-05-19 11:54:35,428 - root - INFO - Epoch 0049 | Average Loss: 4.6734\n100%|██████████| 391/391 [05:50<00:00,  1.12it/s]\n2025-05-19 12:00:25,476 - root - INFO - Epoch 0050 | Average Loss: 4.6880\n100%|██████████| 782/782 [03:37<00:00,  3.59it/s]\n2025-05-19 12:04:03,391 - root - INFO - Val: Epoch 0050 | Recall [0.3651, 0.7045, 0.8405], NDCG [0.3651, 0.5429 0.5873]\n2025-05-19 12:04:03,418 - root - INFO - Save model at epoch 0050!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"trainer.test()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T12:05:11.599263Z","iopub.execute_input":"2025-05-19T12:05:11.600157Z","iopub.status.idle":"2025-05-19T12:07:22.717451Z","shell.execute_reply.started":"2025-05-19T12:05:11.600127Z","shell.execute_reply":"2025-05-19T12:07:22.716585Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/220391116.py:168: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  best_model = torch.load(os.path.join(self.args.save_dir, 'model', 'best_model.pth')).get('model_state_dict')\n","output_type":"stream"},{"name":"stdout","text":"All logs will be saved to /kaggle/working/log0.log\nTest best model with test set!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [02:11<00:00,  5.97it/s]\n2025-05-19 12:07:22,713 - root - INFO - Test: Recall [0.2704, 0.6005, 0.7660], NDCG [0.2704, 0.4418 0.4955]\n","output_type":"stream"}],"execution_count":11}]}