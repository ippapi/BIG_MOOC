{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11764043,"sourceType":"datasetVersion","datasetId":7380755},{"sourceId":403142,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":323629,"modelId":344419},{"sourceId":404441,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":323629,"modelId":344419},{"sourceId":405327,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":323629,"modelId":344419}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport logging\nimport csv\nfrom collections import OrderedDict\n\n\ndef create_log_id(dir_path):\n    log_count = 0\n    file_path = os.path.join(dir_path, 'log{:d}.log'.format(log_count))\n    while os.path.exists(file_path):\n        log_count += 1\n        file_path = os.path.join(dir_path, 'log{:d}.log'.format(log_count))\n    return log_count\n\n\ndef logging_config(folder=None, name=None,\n                   level=logging.DEBUG,\n                   console_level=logging.DEBUG,\n                   no_console=True):\n\n    if not os.path.exists(folder):\n        os.makedirs(folder)\n    for handler in logging.root.handlers:\n        logging.root.removeHandler(handler)\n    logging.root.handlers = []\n    logpath = os.path.join(folder, name + \".log\")\n    print(\"All logs will be saved to %s\" %logpath)\n\n    logging.root.setLevel(level)\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    logfile = logging.FileHandler(logpath)\n    logfile.setLevel(level)\n    logfile.setFormatter(formatter)\n    logging.root.addHandler(logfile)\n\n    if not no_console:\n        logconsole = logging.StreamHandler()\n        logconsole.setLevel(console_level)\n        logconsole.setFormatter(formatter)\n        logging.root.addHandler(logconsole)\n    return folder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T06:42:03.104712Z","iopub.execute_input":"2025-05-21T06:42:03.105799Z","iopub.status.idle":"2025-05-21T06:42:03.121576Z","shell.execute_reply.started":"2025-05-21T06:42:03.105761Z","shell.execute_reply":"2025-05-21T06:42:03.120802Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nfrom collections import OrderedDict\n\nimport torch\n\ndef early_stopping(recall_list, stopping_steps):\n    best_recall = max(recall_list)\n    best_step = recall_list.index(best_recall)\n    if len(recall_list) - best_step - 1 >= stopping_steps:\n        should_stop = True\n    else:\n        should_stop = False\n    return best_recall, should_stop\n\n\ndef save_model(model, model_dir, current_epoch, last_best_epoch=None):\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    model_state_file = os.path.join(model_dir, 'model_epoch{}.pth'.format(current_epoch))\n    torch.save({'model_state_dict': model.state_dict(), 'epoch': current_epoch}, model_state_file)\n\n    if last_best_epoch is not None and current_epoch != last_best_epoch:\n        old_model_state_file = os.path.join(model_dir, 'model_epoch{}.pth'.format(last_best_epoch))\n        if os.path.exists(old_model_state_file):\n            os.system('rm {}'.format(old_model_state_file))\n            \ndef save_checkpoint(model_dir, model, optimizer, current_epoch, best_recall, best_epoch, metrics_list, epoch_list):\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    checkpoint_file = os.path.join(model_dir, 'checkpoint_epoch{}.pth'.format(current_epoch))\n    torch.save({'model_state_dict': model.state_dict(), \n                'optimizer_state_dict': optimizer.state_dict(), \n                'epoch': current_epoch,\n                'best_recall': best_recall,\n                'best_epoch': best_epoch,\n                'metrics_list': metrics_list,\n                'epoch_list': epoch_list\n               }, checkpoint_file)\n\ndef load_model(model, model_path):\n    checkpoint = torch.load(model_path, map_location=torch.device('cpu'), weights_only=False)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.eval()\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T06:42:03.122750Z","iopub.execute_input":"2025-05-21T06:42:03.123175Z","iopub.status.idle":"2025-05-21T06:42:03.147146Z","shell.execute_reply.started":"2025-05-21T06:42:03.123112Z","shell.execute_reply":"2025-05-21T06:42:03.146183Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score, log_loss, mean_squared_error\n\n\ndef calc_recall(rank, ground_truth, k):\n    \"\"\"\n    calculate recall of one example\n    \"\"\"\n    return len(set(rank[:k]) & set(ground_truth)) / float(len(set(ground_truth)))\n\n\ndef precision_at_k(hit, k):\n    \"\"\"\n    calculate Precision@k\n    hit: list, element is binary (0 / 1)\n    \"\"\"\n    hit = np.asarray(hit)[:k]\n    return np.mean(hit)\n\n\ndef precision_at_k_batch(hits, k):\n    \"\"\"\n    calculate Precision@k\n    hits: array, element is binary (0 / 1), 2-dim\n    \"\"\"\n    res = hits[:, :k].mean(axis=1)\n    return res\n\n\ndef average_precision(hit, cut):\n    \"\"\"\n    calculate average precision (area under PR curve)\n    hit: list, element is binary (0 / 1)\n    \"\"\"\n    hit = np.asarray(hit)\n    precisions = [precision_at_k(hit, k + 1) for k in range(cut) if len(hit) >= k]\n    if not precisions:\n        return 0.\n    return np.sum(precisions) / float(min(cut, np.sum(hit)))\n\n\ndef dcg_at_k(rel, k):\n    \"\"\"\n    calculate discounted cumulative gain (dcg)\n    rel: list, element is positive real values, can be binary\n    \"\"\"\n    rel = np.asfarray(rel)[:k]\n    dcg = np.sum((2 ** rel - 1) / np.log2(np.arange(2, rel.size + 2)))\n    return dcg\n\n\ndef ndcg_at_k(rel, k):\n    \"\"\"\n    calculate normalized discounted cumulative gain (ndcg)\n    rel: list, element is positive real values, can be binary\n    \"\"\"\n    idcg = dcg_at_k(sorted(rel, reverse=True), k)\n    if not idcg:\n        return 0.\n    return dcg_at_k(rel, k) / idcg\n\n\ndef ndcg_at_k_batch(hits, k):\n    \"\"\"\n    calculate NDCG@k\n    hits: array, element is binary (0 / 1), 2-dim\n    \"\"\"\n    hits_k = hits[:, :k]\n    dcg = np.sum((2 ** hits_k - 1) / np.log2(np.arange(2, k + 2)), axis=1)\n\n    sorted_hits_k = np.flip(np.sort(hits), axis=1)[:, :k]\n    idcg = np.sum((2 ** sorted_hits_k - 1) / np.log2(np.arange(2, k + 2)), axis=1)\n\n    idcg[idcg == 0] = np.inf\n    ndcg = (dcg / idcg)\n    return ndcg\n\n\ndef recall_at_k(hit, k, all_pos_num):\n    \"\"\"\n    calculate Recall@k\n    hit: list, element is binary (0 / 1)\n    \"\"\"\n    hit = np.asfarray(hit)[:k]\n    return np.sum(hit) / all_pos_num\n\n\ndef recall_at_k_batch(hits, k):\n    \"\"\"\n    calculate Recall@k\n    hits: array, element is binary (0 / 1), 2-dim\n    \"\"\"\n    res = (hits[:, :k].sum(axis=1) / hits.sum(axis=1))\n    return res\n\n\ndef F1(pre, rec):\n    if pre + rec > 0:\n        return (2.0 * pre * rec) / (pre + rec)\n    else:\n        return 0.\n\n\ndef calc_auc(ground_truth, prediction):\n    try:\n        res = roc_auc_score(y_true=ground_truth, y_score=prediction)\n    except Exception:\n        res = 0.\n    return res\n\n\ndef logloss(ground_truth, prediction):\n    logloss = log_loss(np.asarray(ground_truth), np.asarray(prediction))\n    return logloss\n\n\ndef calc_metrics_at_k(cf_scores, train_user_dict, test_user_dict, user_ids, item_ids, Ks, num_negatives=100):\n    '''\n    Calculate precision, recall, and NDCG at K for each user with negative sampling.\n    Negative sampling selects 100 items that are neither the ground truth item nor in the user's train set.\n\n    cf_scores: (n_users, n_items)\n    '''\n\n    binary_hit = []\n    temp_cf_scores = []\n    test_indices = []\n\n    test_pos_item_binary = np.concatenate((\n        np.ones((len(user_ids), 1)),\n        np.zeros((len(user_ids), num_negatives))\n    ), axis=1)\n\n\n    for idx, user in enumerate(user_ids):\n        # Ground truth items for the user\n        test_item = set(test_user_dict[user])\n\n        # Items in the training set to be excluded\n        train_items = set(train_user_dict[user])\n\n        # Negative samples: items not in the test items and not in the train items\n        possible_negatives = [item for item in item_ids if item not in train_items and item not in test_item]\n        negative_samples = np.random.choice(possible_negatives, num_negatives, replace=False)\n\n        # Selected items for testing: ground truth + negative samples\n        test_set = list(test_item) + list(negative_samples)\n        # test_indices.append(test_set)\n\n        # Get the corresponding scores of these items from the cf_scores matrix\n        temp_cf_scores.append(cf_scores[idx][test_set].tolist())\n\n    try:\n        _, rank_indices = torch.sort(torch.LongTensor(temp_cf_scores).cuda(), descending=True)    # try to speed up the sorting process\n    except:\n        _, rank_indices = torch.sort(torch.LongTensor(temp_cf_scores), descending=True)\n\n    rank_indices = rank_indices.cpu()\n\n    # binary_hit = [] # shape (n_users, num_negatives+1)\n    # test_indices = np.asarray(test_indices)\n\n    for i in range(len(user_ids)):\n        binary_hit.append(test_pos_item_binary[i][rank_indices[i]])\n    binary_hit = np.array(binary_hit, dtype=np.float32)\n\n    metrics_dict = {}\n    for k in Ks:\n        metrics_dict[k] = {}\n        metrics_dict[k]['precision'] = precision_at_k_batch(binary_hit, k)\n        metrics_dict[k]['recall']    = recall_at_k_batch(binary_hit, k)\n        metrics_dict[k]['ndcg']      = ndcg_at_k_batch(binary_hit, k)\n\n    return metrics_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T06:42:03.149531Z","iopub.execute_input":"2025-05-21T06:42:03.149836Z","iopub.status.idle":"2025-05-21T06:42:03.718088Z","shell.execute_reply.started":"2025-05-21T06:42:03.149810Z","shell.execute_reply":"2025-05-21T06:42:03.717204Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nimport os\nimport random\nimport scipy.sparse as sp\nimport ast\nfrom torch.utils.data import Dataset, DataLoader\n\nclass DataBuilderFM(object):\n    def __init__(self, args, logging):\n        self.args = args\n        self.data_dir = args.data_dir\n        self.train_file = os.path.join(self.data_dir, 'train_df.csv')\n        self.test_file = os.path.join(\n            self.data_dir,\n            'val_df.csv' if self.args.mode == 'train' else 'test_df.csv'\n        )\n        self.kg_file = os.path.join(self.data_dir, \"kg_final.txt\")\n        self.user_file = os.path.join(self.data_dir, \"user_list.txt\")\n\n        self.train_batch_size = args.train_batch_size\n        self.test_batch_size = args.test_batch_size\n\n        kg_data = self.load_kg(self.kg_file)\n        users_info = self.load_user_info(self.user_file)\n        self.cf_train_data, self.train_user_dict = self.load_train_cf(self.train_file)\n        self.cf_test_data, self.test_user_dict = self.load_test_cf(self.test_file)\n        self.statistic_cf()\n\n        self.construct_data(kg_data, users_info)\n        self.print_info(logging)\n\n    def load_train_cf(self, filename):\n        user = []\n        item = []\n        user_dict = dict()\n    \n        df = pd.read_csv(filename)\n    \n        for _, row in df.iterrows():\n            user_id = int(row['user'])\n    \n            item_ids = list(set(ast.literal_eval(row['feature'])))\n    \n            for item_id in item_ids:\n                user.append(user_id)\n                item.append(item_id)\n            user_dict[user_id] = item_ids\n    \n        user = np.array(user, dtype=np.int32)\n        item = np.array(item, dtype=np.int32)\n        return (user, item), user_dict\n\n    def load_test_cf(self, filename):\n        user = []\n        item = []\n        user_dict = dict()\n    \n        df = pd.read_csv(filename, header=0, names=['user', 'label', 'time'])\n    \n        for _, row in df.iterrows():\n            user_id = int(row['user'])\n            item_id = int(row['label'])\n\n            user.append(user_id)\n            item.append(item_id)\n            if user_id not in user_dict:\n                user_dict[user_id] = []\n            user_dict[user_id].append(item_id)\n    \n        user = np.array(user, dtype=np.int32)\n        item = np.array(item, dtype=np.int32)\n        return (user, item), user_dict\n\n\n    def statistic_cf(self):\n        self.n_users = max(max(self.cf_train_data[0]), max(self.cf_test_data[0])) + 1\n        self.n_items = max(max(self.cf_train_data[1]), max(self.cf_test_data[1])) + 1\n        self.n_cf_train = len(self.cf_train_data[0])\n        self.n_cf_test = len(self.cf_test_data[0])\n\n\n    def load_kg(self, filename):\n        kg_data = pd.read_csv(filename, sep='\\t', names=['h', 'r', 't'], engine='python')\n        kg_data = kg_data.drop_duplicates()\n        return kg_data\n\n    def load_user_info(self, filename):\n        user_data = pd.read_csv(filename, sep=' ')\n        user_data = user_data.drop_duplicates()\n        return user_data\n\n    def construct_data(self, kg_data, users_info):\n        # construct user matrix\n        feat_rows = list(range(self.n_users))\n        feat_cols = list(range(self.n_users))\n        feat_data = [1] * self.n_users\n\n        self.n_user_attr = self.n_users\n\n        if users_info is not None:\n            user_cols = [col for col in users_info.columns\n                             if col not in ['id', 'remap_id']]\n            \n            for col in user_cols:\n                feat_rows += list(range(self.n_users))\n                feat_cols += (users_info[col] + self.n_user_attr).to_list()\n                feat_data += [1] * users_info.shape[0]\n                self.n_user_attr += max(users_info[col]) + 1\n\n        self.user_matrix = sp.coo_matrix((feat_data, (feat_rows, feat_cols)), shape=(self.n_users, self.n_user_attr)).tocsr()\n\n        # construct feature matrix\n        self.n_entities = max(max(kg_data['h']), max(kg_data['t'])) + 1\n\n        feat_rows = list(range(self.n_items))\n        feat_cols = list(range(self.n_items))\n        feat_data = [1] * self.n_items\n\n        filtered_kg_data = kg_data[kg_data['h'] < self.n_items]\n        feat_rows += filtered_kg_data['h'].tolist()\n        feat_cols += filtered_kg_data['t'].tolist()\n        feat_data += [1] * filtered_kg_data.shape[0]\n\n        self.feat_matrix = sp.coo_matrix((feat_data, (feat_rows, feat_cols)), shape=(self.n_items, self.n_entities)).tocsr()\n\n        self.n_users_entities = self.n_user_attr + self.n_entities\n\n    def print_info(self, logging):\n        logging.info('n_users:              %d' % self.n_users)\n        logging.info('n_items:              %d' % self.n_items)\n        logging.info('n_entities:           %d' % self.n_entities)\n        logging.info('n_user_attr:           %d' % self.n_user_attr)\n        logging.info('n_users_entities:     %d' % self.n_users_entities)\n\n        logging.info('n_cf_train:           %d' % self.n_cf_train)\n        logging.info('n_cf_test:            %d' % self.n_cf_test)\n\n        logging.info('shape of user_matrix: {}'.format(self.user_matrix.shape))\n        logging.info('shape of feat_matrix: {}'.format(self.feat_matrix.shape))\n\n\nclass TrainDatasetFM(Dataset):\n    def __init__(self, user_dict):\n        self.all_users = list(user_dict.keys())\n\n    def __len__(self):\n        return len(self.all_users)\n\n    def __getitem__(self, idx):\n        user = self.all_users[idx]\n        return user\n\n\ndef process_user_batch(batch_user, user_dict, user_matrix, feat_matrix):\n    def sample_pos_items_for_u(user_dict, u, num_samples=1):\n        pos_items = user_dict[u]\n        return random.sample(pos_items, num_samples)\n    \n    def sample_neg_items_for_u(user_dict, u, num_samples=1, all_item_ids=None):\n        pos_items = set(user_dict[u])\n        neg_items = []\n        while len(neg_items) < num_samples:\n            item = random.choice(all_item_ids)\n            if item not in pos_items:\n                neg_items.append(item)\n        return neg_items\n\n    batch_user = batch_user.tolist()  # Tensor to list if needed\n\n    # To get all item IDs once\n    all_item_ids = list(range(feat_matrix.shape[0]))\n\n    pos_items, neg_items = [], []\n\n    for u in batch_user:\n        pos_items += sample_pos_items_for_u(user_dict, u, 1)\n        neg_items += sample_neg_items_for_u(user_dict, u, 1, all_item_ids=all_item_ids)\n\n    batch_user_np = np.array(batch_user)\n    batch_pos_item_np = np.array(pos_items)\n    batch_neg_item_np = np.array(neg_items)\n\n\n    user_features = user_matrix[batch_user_np]             # shape: (B, user_feat_dim)\n    pos_item_features = feat_matrix[batch_pos_item_np]     # shape: (B, item_feat_dim)\n    neg_item_features = feat_matrix[batch_neg_item_np]     # shape: (B, item_feat_dim)\n\n    pos_feature_values = sp.hstack([user_features, pos_item_features])\n    neg_feature_values = sp.hstack([user_features, neg_item_features])\n\n    return pos_feature_values, neg_feature_values\n\n\n\ndef generate_test_batch(batch_user, n_items, user_matrix, feat_matrix):\n    rep_batch_user = np.repeat(batch_user, n_items)\n    batch_user_sp = user_matrix[rep_batch_user]\n\n    batch_item_sp = sp.vstack([feat_matrix] * len(batch_user))\n\n    feature_values = sp.hstack([batch_user_sp, batch_item_sp])\n    return  feature_values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T06:42:03.719469Z","iopub.execute_input":"2025-05-21T06:42:03.719970Z","iopub.status.idle":"2025-05-21T06:42:04.224805Z","shell.execute_reply.started":"2025-05-21T06:42:03.719934Z","shell.execute_reply":"2025-05-21T06:42:04.223902Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nimport numpy as np\nimport scipy.sparse as sp\n\nclass FM(nn.Module):\n\n    def __init__(self, args,\n                 n_users, n_items, n_entities, n_user_attr,\n                 user_pre_embed=None, item_pre_embed=None):\n\n        super(FM, self).__init__()\n        self.preload = args.preload\n        self.n_users = n_users\n        self.n_items = n_items\n        self.n_user_attr = n_user_attr\n        self.n_entities = n_entities\n        self.n_features = n_user_attr + n_entities\n\n        self.embed_dim = args.embed_dim\n        self.l2loss_lambda = args.l2loss_lambda\n\n        self.linear = nn.Linear(self.n_features, 1)\n        nn.init.xavier_uniform_(self.linear.weight)\n\n        self.feature_embed = nn.Parameter(torch.Tensor(self.n_features, self.embed_dim))\n        nn.init.xavier_uniform_(self.feature_embed)\n\n        self.h = nn.Linear(self.embed_dim, 1, bias=False)\n        with torch.no_grad():\n            self.h.weight.copy_(torch.ones([1, self.embed_dim]))\n        for param in self.h.parameters():\n            param.requires_grad = False\n\n    def convert_coo2tensor(self, coo):\n        values = coo.data\n        indices = np.vstack((coo.row, coo.col))\n\n        i = torch.LongTensor(indices)\n        v = torch.FloatTensor(values)\n        shape = coo.shape\n        return torch.sparse_coo_tensor(i, v, torch.Size(shape)).coalesce()\n\n    def calc_score(self, feature_values):\n        \"\"\"\n        feature_values:  (batch_size, n_features), n_features = n_users + n_entities, torch.sparse.FloatTensor\n        \"\"\"\n        # Bi-Interaction layer\n        # Equation (4) / (3)\n        feature_values = self.convert_coo2tensor(feature_values.tocoo())\n        sum_square_embed = torch.mm(feature_values, self.feature_embed).pow(2)           # (batch_size, embed_dim)\n        square_sum_embed = torch.mm(feature_values.pow(2), self.feature_embed.pow(2))    # (batch_size, embed_dim)\n        z = 0.5 * (sum_square_embed - square_sum_embed)                                  # (batch_size, embed_dim)\n\n        # Prediction layer\n        # Equation (6)\n        y = self.h(z)                                       # (batch_size, 1)\n        # Equation (2) / (7) / (8)\n        y = self.linear(feature_values) + y                 # (batch_size, 1)\n        return y.squeeze()\n\n    def calc_loss(self, pos_feature_values, neg_feature_values):\n        \"\"\"\n        pos_feature_values:  (batch_size, n_features), torch.sparse.FloatTensor\n        neg_feature_values:  (batch_size, n_features), torch.sparse.FloatTensor\n        \"\"\"\n        pos_scores = self.calc_score(pos_feature_values)            # (batch_size)\n        neg_scores = self.calc_score(neg_feature_values)            # (batch_size)\n\n        loss = (-1.0) * torch.log(1e-10 + F.sigmoid(pos_scores - neg_scores))\n        loss = torch.mean(loss)\n\n        l2_loss = torch.norm(self.h.weight, 2).pow(2) / 2\n        loss += self.l2loss_lambda * l2_loss\n        return loss\n\n\n    def forward(self, *input, is_train):\n        if is_train:\n            return self.calc_loss(*input)\n        else:\n            return self.calc_score(*input)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T06:42:04.225687Z","iopub.execute_input":"2025-05-21T06:42:04.226164Z","iopub.status.idle":"2025-05-21T06:42:04.239804Z","shell.execute_reply.started":"2025-05-21T06:42:04.226138Z","shell.execute_reply":"2025-05-21T06:42:04.238668Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import sys\nimport random\nimport itertools\nfrom time import time\nimport datetime\nimport pandas as pd\nfrom tqdm import tqdm\nimport scipy.sparse as sp\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\n\ndef evaluate(args, model, databuilder, Ks):\n    test_batch_size = databuilder.test_batch_size\n    train_user_dict = databuilder.train_user_dict\n    test_user_dict = databuilder.test_user_dict\n\n    model.eval()\n\n    user_ids = list(test_user_dict.keys())\n    user_ids_batches = [user_ids[i: i + test_batch_size] for i in range(0, len(user_ids), test_batch_size)]\n\n    n_users = len(user_ids)\n    n_items = databuilder.n_items\n    item_ids = list(range(n_items))\n    user_idx_map = dict(zip(user_ids, range(n_users)))\n\n    cf_users = []\n    cf_items = []\n    cf_scores = []\n\n    with tqdm(total=len(user_ids_batches), desc='Evaluating Iteration') as pbar:\n        for batch_user in user_ids_batches:\n            feature_values = generate_test_batch(batch_user, databuilder.n_items, databuilder.user_matrix, databuilder.feat_matrix)\n\n            with torch.no_grad():\n                batch_scores = model(feature_values, is_train=False)            # (batch_size)\n\n            cf_users.extend(np.repeat(batch_user, n_items).tolist())\n            cf_items.extend(item_ids * len(batch_user))\n            cf_scores.append(batch_scores.cpu())\n            pbar.update(1)\n\n    rows = [user_idx_map[u] for u in cf_users]\n    cols = cf_items\n    cf_scores = torch.cat(cf_scores)\n    cf_score_matrix = torch.Tensor(sp.coo_matrix((cf_scores, (rows, cols)), shape=(n_users, n_items)).todense())\n\n    user_ids = np.array(user_ids)\n    item_ids = np.array(item_ids)\n    metrics_dict = calc_metrics_at_k(cf_score_matrix, train_user_dict, test_user_dict, user_ids, item_ids, Ks)\n\n    cf_score_matrix = cf_score_matrix.numpy()\n    for k in Ks:\n        for m in ['precision', 'recall', 'ndcg']:\n            metrics_dict[k][m] = metrics_dict[k][m].mean()\n    return cf_score_matrix, metrics_dict\n\ndef train(args):\n    dist.init_process_group(\"gloo\", timeout=datetime.timedelta(seconds=7200))\n    rank = dist.get_rank()\n\n    # Seed đồng bộ trên tất cả process\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    \n    torch.manual_seed(args.seed)\n\n    # Chỉ rank 0 log thông tin\n    if rank == 0:\n        log_save_id = create_log_id(args.save_dir)\n        logging_config(folder=args.save_dir, name=f'log{log_save_id}', no_console=False)\n        logging.info(args)\n\n    # Load data + DistributedSampler\n    data_builder = DataBuilderFM(args, logging)\n    train_dataset = TrainDatasetFM(data_builder.train_user_dict)\n    \n    sampler = DistributedSampler(\n        train_dataset,\n        shuffle=True\n    )\n    dataloader = DataLoader(\n        train_dataset,\n        batch_size=data_builder.train_batch_size // dist.get_world_size(),\n        sampler=sampler,\n        num_workers=0,\n        drop_last=True\n    )\n\n    model = FM(args, data_builder.n_users, data_builder.n_items, data_builder.n_entities, data_builder.n_user_attr)\n    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n    initial_epoch = 1\n    \n    if args.preload == 1:\n        checkpoint = torch.load(args.checkpoint_path, weights_only=False)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        initial_epoch = checkpoint['epoch'] + 1\n\n    model = DDP(model)\n\n    if rank == 0:\n        logging.info(model)\n\n    if rank == 0:\n        best_epoch = -1\n        best_recall = 0\n        Ks = eval(args.Ks)\n        k_min = min(Ks)\n        k_max = max(Ks)\n        epoch_list = []\n        metrics_list = {k: {'precision': [], 'recall': [], 'ndcg': []} for k in Ks}\n\n        if args.preload == 1:\n            best_epoch = checkpoint['best_epoch']\n            best_recall = checkpoint['best_recall']\n            epoch_list = checkpoint['epoch_list']\n            metrics_list = checkpoint['metrics_list']\n\n    # Huấn luyện\n    steps_per_epoch = data_builder.n_cf_train // data_builder.train_batch_size\n    \n    for epoch in range(initial_epoch, args.n_epoch + 1):\n        model.train()\n        dataloader.sampler.set_epoch(epoch)  \n\n        total_loss = 0.0\n        dataloader_iter = iter(dataloader)\n\n        for step in range(steps_per_epoch):\n            try:\n                batch_user = next(dataloader_iter)\n            except StopIteration:\n                dataloader_iter = iter(dataloader)\n                batch_user = next(dataloader_iter)\n    \n            pos_feature_values, neg_feature_values = process_user_batch(\n                batch_user=batch_user,\n                user_dict=data_builder.train_user_dict,\n                user_matrix=data_builder.user_matrix,\n                feat_matrix=data_builder.feat_matrix\n            )\n            \n            batch_loss = model(pos_feature_values, neg_feature_values, is_train=True)\n            \n            if torch.isnan(batch_loss).any():\n                logging.error(f'ERROR: Epoch {epoch} Loss is nan.')\n                sys.exit()\n            batch_loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            total_loss += batch_loss.item()\n\n        average_loss = total_loss / steps_per_epoch\n        \n        average_loss = torch.tensor(average_loss).to(args.device)\n        dist.all_reduce(average_loss, op=dist.ReduceOp.SUM)\n        average_loss = average_loss.item() / dist.get_world_size()\n\n        if rank == 0:\n            logging.info(f'Epoch {epoch:04d} | Average Loss: {average_loss:.4f}')\n\n        dist.barrier()\n        # Đánh giá (chỉ rank 0)\n        if rank == 0 and (epoch % args.evaluate_every == 0 or epoch == args.n_epoch):\n            _, metrics_dict = evaluate(args, model.module, data_builder, Ks)\n\n            # Log và lưu metrics\n            logging.info('CF Evaluation: Epoch {:04d} | Precision [{:.4f}, {:.4f}], Recall [{:.4f}, {:.4f}], NDCG [{:.4f}, {:.4f}]'.format(\n                epoch, metrics_dict[k_min]['precision'], metrics_dict[k_max]['precision'], metrics_dict[k_min]['recall'], metrics_dict[k_max]['recall'], metrics_dict[k_min]['ndcg'], metrics_dict[k_max]['ndcg']))\n\n            epoch_list.append(epoch)\n            for k in Ks:\n                for m in ['precision', 'recall', 'ndcg']:\n                    metrics_list[k][m].append(metrics_dict[k][m])\n\n            # Early stopping\n            best_recall, should_stop = early_stopping(metrics_list[k_max]['recall'], args.stopping_steps)\n            if should_stop:\n                break\n\n            if metrics_list[Ks[-1]]['recall'][-1] == best_recall:\n                save_model(model.module, args.save_dir, epoch, best_epoch)\n                logging.info(f'Save model at epoch {epoch:04d}!')\n                best_epoch = epoch\n                \n        if rank == 0 and (epoch % args.checkpoint_every == 0 or epoch == args.n_epoch):\n            save_checkpoint(args.save_dir, model.module, optimizer, epoch, best_recall, best_epoch, metrics_list, epoch_list)\n        dist.barrier()\n    # Lưu kết quả cuối cùng (rank 0)\n    if rank == 0:\n        metrics_df = [epoch_list]\n        metrics_cols = ['epoch_idx']\n        for k in Ks:\n            for m in ['precision', 'recall', 'ndcg']:\n                metrics_df.append(metrics_list[k][m])\n                metrics_cols.append('{}@{}'.format(m, k))\n        metrics_df = pd.DataFrame(metrics_df).transpose()\n        metrics_df.columns = metrics_cols\n        metrics_df.to_csv(args.save_dir + '/metrics.csv', index=False)\n\n        best_metrics = metrics_df.loc[metrics_df['epoch_idx'] == best_epoch].iloc[0].to_dict()\n        logging.info('Best CF Evaluation: Epoch {:04d} | Precision [{:.4f}, {:.4f}], Recall [{:.4f}, {:.4f}], NDCG [{:.4f}, {:.4f}]'.format(\n            int(best_metrics['epoch_idx']), best_metrics['precision@{}'.format(k_min)], best_metrics['precision@{}'.format(k_max)], best_metrics['recall@{}'.format(k_min)], best_metrics['recall@{}'.format(k_max)], best_metrics['ndcg@{}'.format(k_min)], best_metrics['ndcg@{}'.format(k_max)]))\n\n    dist.destroy_process_group()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T06:42:12.705162Z","iopub.execute_input":"2025-05-21T06:42:12.705601Z","iopub.status.idle":"2025-05-21T06:42:12.739840Z","shell.execute_reply.started":"2025-05-21T06:42:12.705563Z","shell.execute_reply":"2025-05-21T06:42:12.738649Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from types import SimpleNamespace\nargs = SimpleNamespace(\n    mode='train',\n    seed=2024,\n    data_dir='/kaggle/input/mooc-fm',\n    preload=0,\n    embed_dim=64,\n    l2loss_lambda=1e-5,\n    train_batch_size=1024,\n    test_batch_size=1024,\n    lr=0.0001,\n    n_epoch=50,\n    stopping_steps=2,\n    checkpoint_every=5,\n    evaluate_every=50,\n    Ks='[1, 5, 10]',\n    save_dir='/kaggle/working/',\n    device='cpu',\n    checkpoint_path='/kaggle/input/fm-distributed-model/pytorch/default/2/checkpoint_epoch30.pth'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T06:42:55.755990Z","iopub.execute_input":"2025-05-21T06:42:55.757502Z","iopub.status.idle":"2025-05-21T06:42:55.763617Z","shell.execute_reply.started":"2025-05-21T06:42:55.757457Z","shell.execute_reply":"2025-05-21T06:42:55.762345Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from pyspark.ml.torch.distributor import TorchDistributor\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n    .appName(\"DistributedTorchTrain\") \\\n    .getOrCreate()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T06:42:26.718674Z","iopub.execute_input":"2025-05-21T06:42:26.719003Z","iopub.status.idle":"2025-05-21T06:42:35.737584Z","shell.execute_reply.started":"2025-05-21T06:42:26.718978Z","shell.execute_reply":"2025-05-21T06:42:35.736398Z"}},"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/05/21 06:42:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"distributor = TorchDistributor(num_processes=2, local_mode=True, use_gpu=False)\ndistributor.run(train, args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T14:34:35.617332Z","iopub.execute_input":"2025-05-13T14:34:35.617780Z","iopub.status.idle":"2025-05-14T00:12:15.877864Z","shell.execute_reply.started":"2025-05-13T14:34:35.617748Z","shell.execute_reply":"2025-05-14T00:12:15.876213Z"}},"outputs":[{"name":"stdout","text":"[W513 14:34:50.298867606 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W513 14:35:00.309617832 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W513 14:35:13.228267404 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W513 14:35:13.294432797 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W513 14:35:23.239109062 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W513 14:35:33.249772110 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\nAll logs will be saved to /kaggle/working/log0.log\n2025-05-13 14:35:33,362 - root - INFO - namespace(mode='train', seed=2024, data_dir='/kaggle/input/mooc-fm', preload=0, embed_dim=64, l2loss_lambda=1e-05, train_batch_size=1024, test_batch_size=1024, lr=0.0001, n_epoch=50, stopping_steps=2, checkpoint_every=5, evaluate_every=50, Ks='[1, 5, 10]', save_dir='/kaggle/working/', device='cpu', checkpoint_path='/kaggle/input/fm-distributed-model/pytorch/default/2/checkpoint_epoch30.pth', pretrain_model_path='/kaggle/input/fm-distributed-model/pytorch/default/2/checkpoint_epoch30.pth')\n2025-05-13 14:35:51,275 - root - INFO - n_users:              99970\n2025-05-13 14:35:51,276 - root - INFO - n_items:              2827\n2025-05-13 14:35:51,276 - root - INFO - n_entities:           10278\n2025-05-13 14:35:51,276 - root - INFO - n_user_attr:           99973\n2025-05-13 14:35:51,276 - root - INFO - n_users_entities:     110251\n2025-05-13 14:35:51,276 - root - INFO - n_cf_train:           1796450\n2025-05-13 14:35:51,276 - root - INFO - n_cf_test:            99970\n2025-05-13 14:35:51,276 - root - INFO - shape of user_matrix: (99970, 99973)\n2025-05-13 14:35:51,276 - root - INFO - shape of feat_matrix: (2827, 10278)\n2025-05-13 14:35:54,890 - root - INFO - DistributedDataParallel(\n  (module): FM(\n    (linear): Linear(in_features=110251, out_features=1, bias=True)\n    (h): Linear(in_features=64, out_features=1, bias=False)\n  )\n)\n2025-05-13 14:46:13,771 - root - INFO - Epoch 0001 | Average Loss: 0.4048\n2025-05-13 14:56:27,621 - root - INFO - Epoch 0002 | Average Loss: 0.3041\n2025-05-13 15:06:37,561 - root - INFO - Epoch 0003 | Average Loss: 0.2892\n2025-05-13 15:16:50,823 - root - INFO - Epoch 0004 | Average Loss: 0.2793\n2025-05-13 15:27:09,218 - root - INFO - Epoch 0005 | Average Loss: 0.2714\n2025-05-13 15:37:33,244 - root - INFO - Epoch 0006 | Average Loss: 0.2656\n2025-05-13 15:47:44,730 - root - INFO - Epoch 0007 | Average Loss: 0.2586\n2025-05-13 15:57:52,637 - root - INFO - Epoch 0008 | Average Loss: 0.2531\n2025-05-13 16:08:17,113 - root - INFO - Epoch 0009 | Average Loss: 0.2467\n2025-05-13 16:18:33,063 - root - INFO - Epoch 0010 | Average Loss: 0.2420\n2025-05-13 16:28:37,954 - root - INFO - Epoch 0011 | Average Loss: 0.2350\n2025-05-13 16:38:47,126 - root - INFO - Epoch 0012 | Average Loss: 0.2301\n2025-05-13 16:48:55,724 - root - INFO - Epoch 0013 | Average Loss: 0.2232\n2025-05-13 16:59:00,912 - root - INFO - Epoch 0014 | Average Loss: 0.2174\n2025-05-13 17:09:10,159 - root - INFO - Epoch 0015 | Average Loss: 0.2113\n2025-05-13 17:19:14,563 - root - INFO - Epoch 0016 | Average Loss: 0.2049\n2025-05-13 17:29:20,170 - root - INFO - Epoch 0017 | Average Loss: 0.1983\n2025-05-13 17:39:36,434 - root - INFO - Epoch 0018 | Average Loss: 0.1918\n2025-05-13 17:49:35,404 - root - INFO - Epoch 0019 | Average Loss: 0.1858\n2025-05-13 17:59:42,746 - root - INFO - Epoch 0020 | Average Loss: 0.1794\n2025-05-13 18:09:52,987 - root - INFO - Epoch 0021 | Average Loss: 0.1742\n2025-05-13 18:20:03,174 - root - INFO - Epoch 0022 | Average Loss: 0.1683\n2025-05-13 18:30:17,470 - root - INFO - Epoch 0023 | Average Loss: 0.1633\n2025-05-13 18:40:26,791 - root - INFO - Epoch 0024 | Average Loss: 0.1575\n2025-05-13 18:50:45,129 - root - INFO - Epoch 0025 | Average Loss: 0.1521\n2025-05-13 19:00:55,642 - root - INFO - Epoch 0026 | Average Loss: 0.1472\n2025-05-13 19:11:05,847 - root - INFO - Epoch 0027 | Average Loss: 0.1425\n2025-05-13 19:21:12,077 - root - INFO - Epoch 0028 | Average Loss: 0.1375\n2025-05-13 19:31:19,862 - root - INFO - Epoch 0029 | Average Loss: 0.1330\n2025-05-13 19:41:18,909 - root - INFO - Epoch 0030 | Average Loss: 0.1287\n2025-05-13 19:51:21,846 - root - INFO - Epoch 0031 | Average Loss: 0.1249\n2025-05-13 20:01:34,086 - root - INFO - Epoch 0032 | Average Loss: 0.1210\n2025-05-13 20:11:33,670 - root - INFO - Epoch 0033 | Average Loss: 0.1174\n2025-05-13 20:21:44,100 - root - INFO - Epoch 0034 | Average Loss: 0.1137\n2025-05-13 20:31:47,978 - root - INFO - Epoch 0035 | Average Loss: 0.1101\n2025-05-13 20:41:54,510 - root - INFO - Epoch 0036 | Average Loss: 0.1074\n2025-05-13 20:52:03,676 - root - INFO - Epoch 0037 | Average Loss: 0.1044\n2025-05-13 21:02:11,147 - root - INFO - Epoch 0038 | Average Loss: 0.1019\n2025-05-13 21:12:19,537 - root - INFO - Epoch 0039 | Average Loss: 0.0993\n2025-05-13 21:22:14,739 - root - INFO - Epoch 0040 | Average Loss: 0.0969\n2025-05-13 21:32:16,721 - root - INFO - Epoch 0041 | Average Loss: 0.0945\n2025-05-13 21:42:16,645 - root - INFO - Epoch 0042 | Average Loss: 0.0922\n2025-05-13 21:52:23,855 - root - INFO - Epoch 0043 | Average Loss: 0.0899\n2025-05-13 22:02:26,203 - root - INFO - Epoch 0044 | Average Loss: 0.0881\n2025-05-13 22:12:33,285 - root - INFO - Epoch 0045 | Average Loss: 0.0859\n2025-05-13 22:22:26,465 - root - INFO - Epoch 0046 | Average Loss: 0.0847\n2025-05-13 22:32:38,589 - root - INFO - Epoch 0047 | Average Loss: 0.0827\n2025-05-13 22:42:43,683 - root - INFO - Epoch 0048 | Average Loss: 0.0814\n2025-05-13 22:52:47,043 - root - INFO - Epoch 0049 | Average Loss: 0.0796\n2025-05-13 23:02:55,277 - root - INFO - Epoch 0050 | Average Loss: 0.0784\nEvaluating Iteration: 100%|██████████| 98/98 [1:06:05<00:00, 40.46s/it]\n2025-05-14 00:12:11,671 - root - INFO - CF Evaluation: Epoch 0050 | Precision [0.2426, 0.0639], Recall [0.2426, 0.6392], NDCG [0.2426, 0.4263]\n2025-05-14 00:12:11,710 - root - INFO - Save model at epoch 0050!\n2025-05-14 00:12:11,869 - root - INFO - Best CF Evaluation: Epoch 0050 | Precision [0.2426, 0.0639], Recall [0.2426, 0.6392], NDCG [0.2426, 0.4263]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from types import SimpleNamespace\nargs = SimpleNamespace(\n    mode='train',\n    seed=2024,\n    data_dir='/kaggle/input/mooc-fm',\n    preload=1,\n    embed_dim=64,\n    l2loss_lambda=1e-5,\n    train_batch_size=1024,\n    test_batch_size=1024,\n    lr=0.0001,\n    n_epoch=150,\n    stopping_steps=2,\n    checkpoint_every=5,\n    evaluate_every=50,\n    Ks='[1, 5, 10]',\n    save_dir='/kaggle/working/',\n    device='cpu',\n    checkpoint_path='/kaggle/input/new_fm_model/pytorch/default/4/checkpoint_epoch100.pth'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T23:36:21.176960Z","iopub.execute_input":"2025-05-19T23:36:21.177342Z","iopub.status.idle":"2025-05-19T23:36:21.183137Z","shell.execute_reply.started":"2025-05-19T23:36:21.177319Z","shell.execute_reply":"2025-05-19T23:36:21.182034Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"from pyspark.ml.torch.distributor import TorchDistributor\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n    .appName(\"DistributedTorchTrain\") \\\n    .getOrCreate()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:47:40.585548Z","iopub.execute_input":"2025-05-20T15:47:40.585893Z","iopub.status.idle":"2025-05-20T15:47:47.944555Z","shell.execute_reply.started":"2025-05-20T15:47:40.585866Z","shell.execute_reply":"2025-05-20T15:47:47.943561Z"}},"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/05/20 15:47:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"distributor = TorchDistributor(num_processes=2, local_mode=True, use_gpu=False)\ndistributor.run(train, args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T23:36:35.987062Z","iopub.execute_input":"2025-05-19T23:36:35.987433Z","iopub.status.idle":"2025-05-20T09:19:22.116859Z","shell.execute_reply.started":"2025-05-19T23:36:35.987409Z","shell.execute_reply":"2025-05-20T09:19:22.114776Z"}},"outputs":[{"name":"stdout","text":"[W519 23:36:49.117494470 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W519 23:36:59.127250923 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W519 23:37:11.921680215 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W519 23:37:11.938077352 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W519 23:37:21.932414797 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W519 23:37:31.942955141 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\nAll logs will be saved to /kaggle/working/log1.log\n2025-05-19 23:37:31,958 - root - INFO - namespace(mode='train', seed=2024, data_dir='/kaggle/input/mooc-fm', preload=1, embed_dim=64, l2loss_lambda=1e-05, train_batch_size=1024, test_batch_size=1024, lr=0.0001, n_epoch=150, stopping_steps=2, checkpoint_every=5, evaluate_every=50, Ks='[1, 5, 10]', save_dir='/kaggle/working/', device='cpu', checkpoint_path='/kaggle/input/new_fm_model/pytorch/default/4/checkpoint_epoch100.pth')\n2025-05-19 23:37:49,080 - root - INFO - n_users:              99970\n2025-05-19 23:37:49,081 - root - INFO - n_items:              2827\n2025-05-19 23:37:49,081 - root - INFO - n_entities:           10278\n2025-05-19 23:37:49,081 - root - INFO - n_user_attr:           99973\n2025-05-19 23:37:49,081 - root - INFO - n_users_entities:     110251\n2025-05-19 23:37:49,081 - root - INFO - n_cf_train:           1796450\n2025-05-19 23:37:49,081 - root - INFO - n_cf_test:            99970\n2025-05-19 23:37:49,081 - root - INFO - shape of user_matrix: (99970, 99973)\n2025-05-19 23:37:49,081 - root - INFO - shape of feat_matrix: (2827, 10278)\n2025-05-19 23:37:50,667 - root - INFO - DistributedDataParallel(\n  (module): FM(\n    (linear): Linear(in_features=110251, out_features=1, bias=True)\n    (h): Linear(in_features=64, out_features=1, bias=False)\n  )\n)\n2025-05-19 23:47:35,343 - root - INFO - Epoch 0101 | Average Loss: 0.0467\n2025-05-19 23:57:01,512 - root - INFO - Epoch 0102 | Average Loss: 0.0462\n2025-05-20 00:06:28,360 - root - INFO - Epoch 0103 | Average Loss: 0.0459\n2025-05-20 00:16:15,445 - root - INFO - Epoch 0104 | Average Loss: 0.0456\n2025-05-20 00:26:05,524 - root - INFO - Epoch 0105 | Average Loss: 0.0453\n2025-05-20 00:35:49,167 - root - INFO - Epoch 0106 | Average Loss: 0.0453\n2025-05-20 00:45:35,903 - root - INFO - Epoch 0107 | Average Loss: 0.0451\n2025-05-20 00:55:19,336 - root - INFO - Epoch 0108 | Average Loss: 0.0445\n2025-05-20 01:05:05,020 - root - INFO - Epoch 0109 | Average Loss: 0.0444\n2025-05-20 01:14:57,401 - root - INFO - Epoch 0110 | Average Loss: 0.0442\n2025-05-20 01:24:43,441 - root - INFO - Epoch 0111 | Average Loss: 0.0440\n2025-05-20 01:34:30,638 - root - INFO - Epoch 0112 | Average Loss: 0.0436\n2025-05-20 01:44:15,733 - root - INFO - Epoch 0113 | Average Loss: 0.0436\n2025-05-20 01:54:00,211 - root - INFO - Epoch 0114 | Average Loss: 0.0430\n2025-05-20 02:03:49,847 - root - INFO - Epoch 0115 | Average Loss: 0.0432\n2025-05-20 02:13:34,364 - root - INFO - Epoch 0116 | Average Loss: 0.0426\n2025-05-20 02:23:22,829 - root - INFO - Epoch 0117 | Average Loss: 0.0427\n2025-05-20 02:33:08,793 - root - INFO - Epoch 0118 | Average Loss: 0.0424\n2025-05-20 02:42:53,493 - root - INFO - Epoch 0119 | Average Loss: 0.0421\n2025-05-20 02:52:39,419 - root - INFO - Epoch 0120 | Average Loss: 0.0418\n2025-05-20 03:02:25,010 - root - INFO - Epoch 0121 | Average Loss: 0.0419\n2025-05-20 03:12:10,713 - root - INFO - Epoch 0122 | Average Loss: 0.0418\n2025-05-20 03:21:58,353 - root - INFO - Epoch 0123 | Average Loss: 0.0417\n2025-05-20 03:31:51,621 - root - INFO - Epoch 0124 | Average Loss: 0.0413\n2025-05-20 03:42:23,200 - root - INFO - Epoch 0125 | Average Loss: 0.0409\n2025-05-20 03:52:53,482 - root - INFO - Epoch 0126 | Average Loss: 0.0408\n2025-05-20 04:03:31,643 - root - INFO - Epoch 0127 | Average Loss: 0.0408\n2025-05-20 04:14:01,237 - root - INFO - Epoch 0128 | Average Loss: 0.0409\n2025-05-20 04:24:41,870 - root - INFO - Epoch 0129 | Average Loss: 0.0404\n2025-05-20 04:35:20,027 - root - INFO - Epoch 0130 | Average Loss: 0.0400\n2025-05-20 04:46:01,278 - root - INFO - Epoch 0131 | Average Loss: 0.0400\n2025-05-20 04:56:33,655 - root - INFO - Epoch 0132 | Average Loss: 0.0398\n2025-05-20 05:07:07,885 - root - INFO - Epoch 0133 | Average Loss: 0.0398\n2025-05-20 05:17:44,991 - root - INFO - Epoch 0134 | Average Loss: 0.0395\n2025-05-20 05:28:25,701 - root - INFO - Epoch 0135 | Average Loss: 0.0393\n2025-05-20 05:39:03,331 - root - INFO - Epoch 0136 | Average Loss: 0.0393\n2025-05-20 05:49:38,026 - root - INFO - Epoch 0137 | Average Loss: 0.0387\n2025-05-20 06:00:17,556 - root - INFO - Epoch 0138 | Average Loss: 0.0387\n2025-05-20 06:11:01,784 - root - INFO - Epoch 0139 | Average Loss: 0.0388\n2025-05-20 06:21:52,597 - root - INFO - Epoch 0140 | Average Loss: 0.0386\n2025-05-20 06:32:30,022 - root - INFO - Epoch 0141 | Average Loss: 0.0383\n2025-05-20 06:43:05,846 - root - INFO - Epoch 0142 | Average Loss: 0.0383\n2025-05-20 06:53:50,925 - root - INFO - Epoch 0143 | Average Loss: 0.0379\n2025-05-20 07:04:35,434 - root - INFO - Epoch 0144 | Average Loss: 0.0382\n2025-05-20 07:15:16,283 - root - INFO - Epoch 0145 | Average Loss: 0.0380\n2025-05-20 07:26:02,766 - root - INFO - Epoch 0146 | Average Loss: 0.0375\n2025-05-20 07:36:52,681 - root - INFO - Epoch 0147 | Average Loss: 0.0378\n2025-05-20 07:47:35,894 - root - INFO - Epoch 0148 | Average Loss: 0.0375\n2025-05-20 07:58:17,861 - root - INFO - Epoch 0149 | Average Loss: 0.0374\n2025-05-20 08:09:09,387 - root - INFO - Epoch 0150 | Average Loss: 0.0374\nEvaluating Iteration: 100%|██████████| 98/98 [1:06:54<00:00, 40.96s/it]\n2025-05-20 09:19:18,556 - root - INFO - CF Evaluation: Epoch 0150 | Precision [0.2425, 0.0669], Recall [0.2425, 0.6691], NDCG [0.2425, 0.4380]\n2025-05-20 09:19:18,602 - root - INFO - Save model at epoch 0150!\n2025-05-20 09:19:18,764 - root - INFO - Best CF Evaluation: Epoch 0150 | Precision [0.2425, 0.0669], Recall [0.2425, 0.6691], NDCG [0.2425, 0.4380]\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"args.pretrain_model_path = \"/kaggle/input/new_fm_model/pytorch/default/6/checkpoint_epoch190.pth\"\nargs.mode = \"test\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T06:43:06.082613Z","iopub.execute_input":"2025-05-21T06:43:06.083459Z","iopub.status.idle":"2025-05-21T06:43:06.088212Z","shell.execute_reply.started":"2025-05-21T06:43:06.083424Z","shell.execute_reply":"2025-05-21T06:43:06.086925Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def predict(args):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    data_builder = DataBuilderFM(args, logging)\n     \n    model = FM(args, data_builder.n_users, data_builder.n_items, data_builder.n_entities, data_builder.n_user_attr)\n    model = load_model(model, args.pretrain_model_path)\n    model.to(device)\n\n    Ks = eval(args.Ks)\n    k_min = min(Ks)\n    k_max = max(Ks)\n    \n    cf_scores, metrics_dict = evaluate(args, model, data_builder, Ks)\n    for k in Ks:\n        print(f'*** CF Evaluation @{k} ***')\n        print(f'Precision@{k}   : ', metrics_dict[k]['precision'])\n        print(f'Recall@{k}      : ', metrics_dict[k]['recall'])\n        print(f'NDCG@{k}        : ', metrics_dict[k]['ndcg'])\n\npredict(args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T06:43:17.947614Z","iopub.execute_input":"2025-05-21T06:43:17.947920Z","iopub.status.idle":"2025-05-21T07:42:48.319610Z","shell.execute_reply.started":"2025-05-21T06:43:17.947898Z","shell.execute_reply":"2025-05-21T07:42:48.317965Z"}},"outputs":[{"name":"stderr","text":"Evaluating Iteration: 100%|██████████| 98/98 [55:49<00:00, 34.18s/it]\n","output_type":"stream"},{"name":"stdout","text":"*** CF Evaluation @1 ***\nPrecision@1   :  0.19940983\nRecall@1      :  0.19940983\nNDCG@1        :  0.19940982294688406\n*** CF Evaluation @5 ***\nPrecision@5   :  0.094080225\nRecall@5      :  0.4704011\nNDCG@5        :  0.3398102717534796\n*** CF Evaluation @10 ***\nPrecision@10   :  0.06079024\nRecall@10      :  0.60790235\nNDCG@10        :  0.38425794189657836\n","output_type":"stream"}],"execution_count":11}]}